<!DOCTYPE html>
<html lang="uk">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Адаптивні методи стеганографії - Детальний конспект | Лекція 7</title>

    <!-- CSS -->
    <link rel="stylesheet" href="../../../assets/css/base.css">
    <link rel="stylesheet" href="../../../assets/css/layout.css">
    <link rel="stylesheet" href="../../../assets/css/components.css">
    <link rel="stylesheet" href="../../../assets/css/lectures.css">

    <style>
        html {
            scroll-behavior: smooth;
        }

        .konspekt-container {
            display: flex;
            max-width: 1400px;
            margin: 0 auto;
            gap: 2rem;
            padding: 2rem;
        }

        .konspekt-page {
            flex: 1;
            max-width: 900px;
            background: white;
            line-height: 1.9;
        }

        /* Right Sidebar Navigation */
        .sidebar-nav {
            position: sticky;
            top: 100px;
            width: 280px;
            height: fit-content;
            background: #f8fafc;
            border: 2px solid #e2e8f0;
            border-radius: 12px;
            padding: 1.5rem;
            align-self: flex-start;
        }

        .sidebar-nav h3 {
            font-size: 1.1rem;
            color: #475569;
            margin: 0 0 1rem 0;
            padding-bottom: 0.8rem;
            border-bottom: 2px solid #cbd5e1;
        }

        .sidebar-nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .sidebar-nav li {
            margin-bottom: 0.5rem;
        }

        .sidebar-nav a {
            display: block;
            color: #64748b;
            text-decoration: none;
            padding: 0.6rem 0.8rem;
            border-radius: 6px;
            font-size: 0.95rem;
            transition: all 0.2s ease;
            border-left: 3px solid transparent;
        }

        .sidebar-nav a:hover {
            background: #e0e7ff;
            color: #667eea;
            border-left-color: #667eea;
        }

        .sidebar-nav a.active {
            background: #e0e7ff;
            color: #667eea;
            font-weight: 600;
            border-left-color: #667eea;
        }

        .sidebar-nav .nav-subsection {
            padding-left: 1rem;
            font-size: 0.85rem;
        }

        /* Reading Progress Bar */
        .reading-progress {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 4px;
            background: #e2e8f0;
            z-index: 9999;
        }

        .reading-progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            width: 0%;
            transition: width 0.1s ease;
        }

        /* Back to top button */
        .back-to-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background: #667eea;
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: none;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
            transition: all 0.3s ease;
            z-index: 1000;
            border: none;
            font-size: 1.5rem;
        }

        .back-to-top:hover {
            background: #5568d3;
            transform: translateY(-5px);
        }

        .back-to-top.show {
            display: flex;
        }

        /* Responsive Design */
        @media (max-width: 1200px) {
            .konspekt-container {
                flex-direction: column;
            }

            .sidebar-nav {
                position: relative;
                top: 0;
                width: 100%;
                margin-bottom: 2rem;
            }

            .konspekt-page {
                max-width: 100%;
            }
        }

        @media (max-width: 768px) {
            .konspekt-container {
                padding: 1rem;
            }

            .two-columns {
                grid-template-columns: 1fr;
            }
        }

        .konspekt-header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 3px solid #667eea;
        }

        .konspekt-title {
            font-size: 2.5rem;
            color: #1e293b;
            margin-bottom: 1rem;
        }

        .konspekt-meta {
            color: #64748b;
            font-size: 1.1rem;
        }

        .konspekt-content h2 {
            color: #667eea;
            font-size: 2rem;
            margin-top: 3rem;
            margin-bottom: 1.5rem;
            padding-bottom: 0.8rem;
            border-bottom: 3px solid #e2e8f0;
        }

        .konspekt-content h3 {
            color: #475569;
            font-size: 1.5rem;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        .konspekt-content h4 {
            color: #64748b;
            font-size: 1.2rem;
            margin-top: 1.5rem;
            margin-bottom: 0.8rem;
        }

        .konspekt-content p {
            font-size: 1.1rem;
            margin-bottom: 1.2rem;
            text-align: justify;
        }

        .konspekt-content ul, .konspekt-content ol {
            font-size: 1.05rem;
            margin-bottom: 1.5rem;
            padding-left: 2rem;
        }

        .konspekt-content li {
            margin-bottom: 0.7rem;
            line-height: 1.7;
        }

        .info-box {
            background: #f0f9ff;
            border-left: 5px solid #0284c7;
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 8px;
        }

        .info-box h4 {
            color: #0369a1;
            margin-top: 0;
        }

        .example-box {
            background: #f0fdf4;
            border-left: 5px solid #10b981;
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 8px;
        }

        .example-box h4 {
            color: #065f46;
            margin-top: 0;
        }

        .warning-box {
            background: #fef3c7;
            border-left: 5px solid #f59e0b;
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 8px;
        }

        .warning-box h4 {
            color: #92400e;
            margin-top: 0;
        }

        .important-box {
            background: #fef2f2;
            border-left: 5px solid #ef4444;
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 8px;
        }

        .important-box h4 {
            color: #991b1b;
            margin-top: 0;
        }

        .simple-formula {
            background: #f8fafc;
            border: 2px solid #cbd5e1;
            padding: 1.2rem;
            margin: 1.5rem 0;
            text-align: center;
            font-size: 1.1rem;
            border-radius: 8px;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 8px;
            overflow: hidden;
        }

        .comparison-table th {
            background: #667eea;
            color: white;
            padding: 1rem;
            text-align: left;
            border: 1px solid #ddd;
        }

        .comparison-table td {
            padding: 0.9rem;
            border: 1px solid #ddd;
            vertical-align: top;
        }

        .comparison-table tr:nth-child(even) {
            background: #f9fafb;
        }

        .comparison-table tr:hover {
            background: #f0f9ff;
        }

        .step-list {
            background: #f8fafc;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1.5rem 0;
        }

        .step-list ol {
            counter-reset: step;
            list-style: none;
            padding-left: 0;
        }

        .step-list li {
            counter-increment: step;
            margin-bottom: 1.2rem;
            padding-left: 2.5rem;
            position: relative;
        }

        .step-list li::before {
            content: counter(step);
            position: absolute;
            left: 0;
            top: 0;
            background: #667eea;
            color: white;
            width: 1.8rem;
            height: 1.8rem;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 0.9rem;
        }

        .two-columns {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            margin: 2rem 0;
        }

        .column-card {
            background: white;
            border: 2px solid #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
        }

        .column-card h4 {
            color: #667eea;
            margin-top: 0;
        }

        .toc {
            background: #f0f9ff;
            border: 2px solid #0284c7;
            padding: 2rem;
            margin: 2rem 0;
            border-radius: 8px;
        }

        .toc h3 {
            color: #0369a1;
            margin-top: 0;
        }

        .toc ul {
            list-style: none;
            padding-left: 0;
        }

        .toc li {
            margin-bottom: 0.5rem;
        }

        .toc a {
            color: #0284c7;
            text-decoration: none;
            font-size: 1.05rem;
        }

        .toc a:hover {
            text-decoration: underline;
        }

        @media print {
            .btn, .header, .sidebar-nav, .back-to-top {
                display: none;
            }

            .konspekt-container {
                display: block;
            }

            .konspekt-page {
                max-width: 100%;
            }
        }
    </style>
</head>
<body>

<!-- Reading Progress Bar -->
<div class="reading-progress">
    <div class="reading-progress-bar"></div>
</div>

<!-- Navigation -->
<nav class="header">
    <div class="container">
        <div class="header-nav">
            <div class="breadcrumb">
                <a href="../../../index.html">🏠 Головна</a>
                <span class="breadcrumb-separator">›</span>
                <a href="../../module2/index.html">Модуль 2</a>
                <span class="breadcrumb-separator">›</span>
                <a href="index.html">Лекція 7</a>
                <span class="breadcrumb-separator">›</span>
                <span>Конспект</span>
            </div>
        </div>
    </div>
</nav>

<!-- Main Content Container -->
<div class="konspekt-container">

    <!-- Right Sidebar Navigation -->
    <aside class="sidebar-nav">
        <h3>📑 Навігація</h3>
        <ul>
            <li><a href="#intro">Вступ</a></li>
            <li><a href="#hugo">1. HUGO</a></li>
            <li><a href="#wow">2. WOW</a></li>
            <li><a href="#suniward">3. S-UNIWARD</a></li>
            <li><a href="#comparison">4. Порівняльний аналіз</a></li>
            <li><a href="#conclusion">Висновки</a></li>
        </ul>
    </aside>

    <main class="konspekt-page">

        <!-- Header -->
        <header class="konspekt-header">
            <h1 class="konspekt-title">Адаптивні методи стеганографії</h1>
            <div class="konspekt-meta">
                Навчальний конспект | Лекція 7 | Модуль 2: Методи просторової області
            </div>
        </header>

        <!-- Table of Contents -->
    <div class="toc">
        <h3>📑 Зміст конспекту</h3>
        <ul>
            <li><a href="#intro">Вступ до адаптивної стеганографії</a></li>
            <li><a href="#hugo">Розділ 1: HUGO - Highly Undetectable steGO</a></li>
            <li><a href="#wow">Розділ 2: WOW - Wavelet Obtained Weights</a></li>
            <li><a href="#suniward">Розділ 3: S-UNIWARD</a></li>
            <li><a href="#comparison">Розділ 4: Порівняльний аналіз</a></li>
            <li><a href="#conclusion">Висновки</a></li>
        </ul>
    </div>

    <!-- Content -->
    <article class="konspekt-content">

        <!-- Вступ -->
        <section id="intro">
            <h2>Вступ до адаптивної стеганографії</h2>

            <h3>Що таке адаптивна стеганографія?</h3>

            <p>
                Уявіть, що ви хочете сховати важливе повідомлення в зображенні. Традиційні методи,
                такі як LSB (метод найменш значущого біта), вбудовують дані рівномірно — кожен піксель має
                однакову ймовірність бути зміненим, незалежно від того, чи це гладке небо, чи текстурований ліс.
            </p>

            <p>
                <strong>Адаптивна стеганографія</strong> працює принципово інакше. Вона "розуміє", що різні частини
                зображення мають різну придатність для приховування даних. Це можна порівняти з тим, як ми
                ховаємо цінні речі: ми не кладемо їх на порожній стіл посеред кімнати, а ховаємо в заповненій
                шухляді серед інших предметів, де їх значно важче помітити.
            </p>

            <div class="info-box">
                <h4>💡 Ключова ідея</h4>
                <p>
                    <strong>Адаптивні методи аналізують локальні властивості зображення</strong> і розумно розподіляють дані:
                    більше інформації вбудовується у складних областях (текстури, деталі, контури) та менше —
                    у простих однорідних зонах (гладкі поверхні, небо, стіни). Це робить приховані дані практично невиявними.
                </p>
            </div>

            <h3>Чому це важливо?</h3>

            <p>Традиційні методи мають фундаментальну проблему:</p>

            <ul>
                <li><strong>Вони залишають статистичні сліди</strong> — зміни видно у гістограмах та розподілах значень пікселів</li>
                <li><strong>Вони не враховують контент</strong> — гладкі області так само міняються, як і текстуровані</li>
                <li><strong>Їх легко виявити</strong> — детектори стегоаналізу мають точність 90-99% для LSB методів</li>
            </ul>

            <p>
                Адаптивні методи вирішують ці проблеми, роблячи зміни "природними" — вони виглядають як
                частина зображення, а не як артефакти вбудовування.
            </p>

            <h3>Три покоління стеганографії</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Покоління</th>
                        <th>Приклади методів</th>
                        <th>Основна ідея</th>
                        <th>Виявлюваність</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>1-ше</strong> (1990-2000)</td>
                        <td>LSB replacement, LSB matching</td>
                        <td>Рівномірне вбудовування без аналізу</td>
                        <td>❌ Висока — легко виявити</td>
                    </tr>
                    <tr>
                        <td><strong>2-ге</strong> (2000-2010)</td>
                        <td>F5, OutGuess, MMx</td>
                        <td>Частотні методи з базовою адаптацією</td>
                        <td>⚠️ Середня</td>
                    </tr>
                    <tr>
                        <td><strong>3-тє</strong> (2010-тепер)</td>
                        <td><strong>HUGO, WOW, S-UNIWARD</strong></td>
                        <td>Повна адаптація на основі моделей</td>
                        <td>✅ Низька — майже невиявні</td>
                    </tr>
                </tbody>
            </table>

            <h3>Три методи цієї лекції</h3>

            <div class="two-columns">
                <div class="column-card">
                    <h4>🎭 HUGO</h4>
                    <p><strong>Підхід:</strong> Статистичне моделювання</p>
                    <p>Аналізує, як детектори "бачать" зображення через SPAM модель, і мінімізує зміни в цих ознаках</p>
                </div>

                <div class="column-card">
                    <h4>🌊 WOW</h4>
                    <p><strong>Підхід:</strong> Напрямкова адаптація</p>
                    <p>Використовує вейвлет-фільтри для виявлення напрямків текстури і уникає змін вздовж них</p>
                </div>
            </div>

            <div class="column-card" style="margin-top: 1rem;">
                <h4>📐 S-UNIWARD</h4>
                <p><strong>Підхід:</strong> Універсальне відносне спотворення</p>
                <p>Використовує багаторівневу вейвлет-декомпозицію для оцінки того, як зміни вплинуть на зображення на різних масштабах</p>
            </div>

            <div class="warning-box">
                <h4>⚠️ Важливо розуміти</h4>
                <p>
                    Всі три методи вирішують одну задачу: <strong>"де в зображенні можна безпечно вбудувати дані?"</strong>
                    Але вони підходять до цього питання з різних сторін, використовуючи різні моделі і критерії.
                </p>
            </div>

        </section>

        <!-- HUGO -->
        <section id="hugo">
            <h2>Розділ 1: HUGO - Highly Undetectable steGO</h2>

            <div class="info-box" style="background: #f0f9ff; border-left: 5px solid #0284c7; padding: 1.5rem; margin-bottom: 2rem;">
                <h4 style="color: #0369a1; margin-bottom: 0.5rem;">📝 Розшифровка абревіатури</h4>
                <p style="margin: 0;"><strong>HUGO</strong> = <strong>H</strong>ighly <strong>U</strong>ndetectable ste<strong>GO</strong></p>
                <p style="margin: 0.5rem 0 0 0;"><strong>Український переклад:</strong> Високо Невиявна СтеганоГрафія</p>
                <p style="margin: 0.5rem 0 0 0; font-style: italic; color: #0369a1;">
                    Назва підкреслює головну мету методу — створити стеганографію, яку практично неможливо виявити
                    сучасними засобами стегоаналізу.
                </p>
            </div>

            <h3>1.1. Проблема традиційних методів</h3>

            <p>
                Щоб зрозуміти, чому потрібен HUGO, спочатку розглянемо, що не так із старими методами.
            </p>

            <div class="example-box">
                <h4>📸 Приклад: Фото пейзажу</h4>
                <p>
                    Маємо фотографію з небом у верхній частині та лісом внизу. LSB метод вбудує дані
                    рівномірно: 50% біт піде в небо, 50% — у ліс.
                </p>
                <p>
                    <strong>Проблема:</strong> Небо дуже гладке (значення пікселів 150-155). Навіть зміна
                    ±1 створює помітні статистичні артефакти. Детектор легко це помітить.
                </p>
                <p>
                    <strong>Що б зробив HUGO:</strong> Проаналізував би, що небо має високу "вартість" модифікації,
                    а ліс — низьку. Вбудував би ~95% даних у ліс і лише ~5% у небо.
                </p>
            </div>

            <h3>1.2. Основна ідея HUGO</h3>

            <p>
                HUGO базується на простій, але потужній ідеї: <strong>"Якщо ми розуміємо принципи роботи детектора,
                ми можемо мінімізувати те, що він здатен виявити"</strong>.
            </p>

            <p>Це можна порівняти з грою в прятки, але з додатковою інформацією:</p>
            <ul>
                <li>Ми точно знаємо, на що шукач звертає увагу (які статистичні ознаки він аналізує)</li>
                <li>Ми цілеспрямовано мінімізуємо зміни саме в цих ознаках</li>
                <li>У результаті шукач не може нас знайти, навіть якщо ретельно шукає</li>
            </ul>

            <div class="info-box">
                <h4>🎯 Головний принцип HUGO</h4>
                <p>
                    Замість того, щоб намагатися "обдурити" детектор обхідними шляхами, HUGO використовує науковий підхід:
                    <strong>він моделює роботу детектора</strong> і вбудовує дані таким чином, щоб детектор бачив
                    мінімальні зміни саме в тих статистичних ознаках, які він досліджує. Це робить стеганографію
                    практично невидимою для існуючих засобів виявлення.
                </p>
            </div>

            <h3>1.3. Аналіз локальної складності</h3>

            <p>
                Перш ніж вбудовувати дані, HUGO аналізує кожен піксель зображення і оцінює його
                "складність" або "текстурованість". Це робиться через модель SPAM.
            </p>

            <p><strong>Що таке "складність" пікселя?</strong></p>

            <p>
                Складність показує, наскільки піксель і його околиця мають різноманітні значення.
                Чим більше варіацій навколо пікселя, тим вище складність.
            </p>

            <div class="two-columns">
                <div class="column-card">
                    <h4>🟦 Низька складність</h4>
                    <p><strong>Приклад:</strong> Небо, стіна, гладка поверхня</p>
                    <p>Пікселі мають схожі значення: 150, 151, 150, 152...</p>
                    <p><strong>Результат:</strong> Будь-яка зміна помітна</p>
                </div>

                <div class="column-card">
                    <h4>🌳 Висока складність</h4>
                    <p><strong>Приклад:</strong> Ліс, трава, текстура тканини</p>
                    <p>Пікселі дуже різні: 120, 145, 98, 167, 134...</p>
                    <p><strong>Результат:</strong> Зміна ±1 непомітна в хаосі</p>
                </div>
            </div>

            <h3>1.4. Модель SPAM (Subtractive Pixel Adjacency Matrix)</h3>

            <p>
                SPAM — це математична модель, яка описує взаємозв'язки між пікселями та їхніми сусідами.
                Хоча назва звучить складно, основна ідея досить проста і елегантна.
            </p>

            <div class="info-box">
                <h4>📊 Теоретичні основи SPAM</h4>
                <p>
                    Модель SPAM (Subtractive Pixel Adjacency Matrix), розроблена Pevný et al. (2010), базується на
                    фундаментальному принципі стегоаналізу: <strong>природні зображення мають характерну статистичну
                    структуру, яка порушується при вбудовуванні прихованих даних</strong>.
                </p>
                <p style="margin-top: 1rem;">
                    Ключова ідея SPAM полягає у тому, що замість аналізу абсолютних значень пікселів, метод досліджує
                    <strong>відносини між сусідніми пікселями</strong>. Природні зображення демонструють характерні
                    паттерни цих відносин — наприклад, сусідні пікселі зазвичай мають схожі значення (плавні переходи),
                    або різко відрізняються (краї об'єктів). Стеганографічне вбудовування порушує ці природні паттерни,
                    створюючи "неприродні" комбінації, які й виявляються детекторами.
                </p>
                <p style="margin-top: 1rem;">
                    SPAM аналізує <strong>пари сусідніх різниць</strong>: якщо піксель A відрізняється від пікселя B на +2,
                    а піксель B від пікселя C на -1, то це створює характерну "пару переходів" (+2, -1). Модель підраховує,
                    скільки разів зустрічається кожна можлива пара, створюючи статистичний "відбиток" зображення з 686 чисел.
                    Цей відбиток для природних зображень має певну структуру, а для стего-зображень — іншу.
                </p>
            </div>

            <p><strong>Як працює SPAM (зрозумілою простою мовою):</strong></p>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Обчислюємо різниці між сусідами</strong><br>
                        Замість того, щоб дивитися на абсолютні значення пікселів (100, 102, 101...),
                        ми дивимося на різниці: (102-100=+2, 101-102=-1...)
                    </li>
                    <li>
                        <strong>Рахуємо, які різниці зустрічаються</strong><br>
                        Створюємо "гістограму різниць": різниця -2 зустрілася 100 разів, +1 — 250 разів, і т.д.
                    </li>
                    <li>
                        <strong>Аналізуємо патерни</strong><br>
                        Природні зображення мають характерні патерни цих різниць. Модифіковані зображення — інші патерни.
                    </li>
                    <li>
                        <strong>Створюємо "відбиток" зображення</strong><br>
                        SPAM дає 686 чисел, які описують статистику зображення. Це як "ДНК" зображення.
                    </li>
                </ol>
            </div>

            <div class="example-box">
                <h4>🔍 Чому це працює?</h4>
                <p>
                    Детектори стегоаналізу теж використовують SPAM! Вони порівнюють SPAM "відбиток"
                    підозрілого зображення з відбитками чистих зображень. Якщо відбитки сильно різняться
                    — це стегоконтейнер.
                </p>
                <p>
                    HUGO знає це і намагається змінити зображення так, щоб SPAM відбиток змінився якомога менше.
                </p>
            </div>

            <h3>1.5. Функція вартості</h3>

            <p>
                Після того, як HUGO знає, як детектор аналізує зображення, він вводить поняття
                <strong>"вартість модифікації"</strong> для кожного пікселя.
            </p>

            <p><strong>Вартість — це міра того, наскільки "дорого" змінити конкретний піксель.</strong></p>

            <div class="info-box">
                <h4>🔬 Теоретичне обгрунтування функції вартості</h4>
                <p>
                    Функція вартості (distortion function) є центральним поняттям у сучасній адаптивній стеганографії.
                    Вона кількісно визначає <strong>вплив модифікації кожного елемента контейнера на виявлюваність</strong>
                    вбудованого повідомлення.
                </p>
                <p style="margin-top: 1rem;">
                    У випадку HUGO функція вартості базується на інтелектуальному принципі: <strong>"якщо ми знаємо, як працює
                    детектор, ми можемо передбачити, які зміни він помітить"</strong>. Детектори стегоаналізу аналізують
                    SPAM-характеристики зображення — 686 чисел, які описують статистичні властивості. HUGO обчислює,
                    як зміна кожного конкретного пікселя вплине на ці 686 чисел.
                </p>
                <p style="margin-top: 1rem;">
                    Процес обчислення вартості можна пояснити таким чином: спочатку HUGO обчислює SPAM "відбиток"
                    оригінального зображення. Потім для кожного пікселя він <strong>симулює</strong>, що станеться, якщо
                    змінити цей піксель на +1 або -1. Він знову обчислює SPAM відбиток для зміненого зображення і
                    порівнює — <strong>наскільки сильно змінився відбиток?</strong> Якщо зміна велика, це означає, що
                    детектор швидше за все помітить модифікацію, тому вартість висока. Якщо зміна мінімальна — вартість низька.
                </p>
                <p style="margin-top: 1rem;">
                    Для кожного пікселя обчислюється два значення вартості (для +1 і -1), і береться менше з них.
                    Таким чином, HUGO знає не тільки <em>де</em> можна вбудовувати, а й <em>у який бік</em> краще
                    змінювати піксель (збільшувати чи зменшувати). Загальне спотворення всього зображення — це просто
                    сума вартостей всіх змінених пікселів.
                </p>
            </div>

            <div class="info-box">
                <h4>💰 Як розуміти вартість?</h4>
                <ul>
                    <li><strong>Висока вартість:</strong> Зміна цього пікселя сильно впливає на SPAM ознаки → детектор помітить</li>
                    <li><strong>Низька вартість:</strong> Зміна майже не впливає на SPAM → детектор не помітить</li>
                </ul>
            </div>

            <p><strong>Як обчислюється вартість (концептуально):</strong></p>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Обчислюємо SPAM для оригінального зображення</strong><br>
                        Отримуємо 686 чисел, які описують зображення зараз
                    </li>
                    <li>
                        <strong>Симулюємо зміну пікселя</strong><br>
                        "А що якщо ми збільшимо цей піксель на +1? Або зменшимо на -1?"
                    </li>
                    <li>
                        <strong>Обчислюємо SPAM знову</strong><br>
                        Отримуємо нові 686 чисел для зміненого зображення
                    </li>
                    <li>
                        <strong>Порівнюємо різницю</strong><br>
                        Вартість = сума змін у всіх 686 числах. Чим більше змін, тим вище вартість
                    </li>
                </ol>
            </div>

            <div class="simple-formula">
                <p><strong>Спрощена формула:</strong></p>
                <p>Вартість = Наскільки змінюється SPAM "відбиток" після модифікації</p>
            </div>

            <h3>1.6. STC - Syndrome Trellis Codes</h3>

            <p>
                Після того, як ми знаємо вартість кожного пікселя, виникає питання:
                <strong>"Як оптимально вбудувати повідомлення?"</strong>
            </p>

            <p>
                Наївний підхід був би: "Візьмемо N найдешевших пікселів і вбудуємо в них повідомлення".
                Але це не оптимально, бо біти повідомлення можуть не збігатися з наявними бітами в цих пікселях.
            </p>

            <p>
                <strong>STC (Syndrome Trellis Codes)</strong> — це розумний алгоритм кодування, який вирішує цю проблему.
            </p>

            <div class="info-box">
                <h4>🎯 Теорія кодування і практичне вбудовування</h4>
                <p>
                    Syndrome Trellis Codes (STC), розроблені Filler et al. у 2011 році, вирішують фундаментальну
                    <strong>проблему оптимального вбудовування</strong>. Суть проблеми: маємо зображення з N пікселів,
                    кожен має свою вартість модифікації, і потрібно вбудувати M біт повідомлення так, щоб загальна
                    вартість була мінімальною.
                </p>
                <p style="margin-top: 1rem;">
                    Чому це складна задача? Розглянемо простий приклад: потрібно вбудувати 3 біти "101". У нас є 10 пікселів
                    з різними вартостями. Наївний підхід: "візьмемо 3 найдешевші пікселі і запишемо туди 1-0-1". Але
                    проблема у тому, що ці 3 пікселі вже можуть мати якісь значення (наприклад, 0-1-0), і щоб отримати
                    1-0-1, нам доведеться змінити їх усі! А це 3 модифікації.
                </p>
                <p style="margin-top: 1rem;">
                    STC вирішує це розумно: він розглядає <strong>всі можливі комбінації змін</strong> і знаходить таку, яка:
                    (1) вбудує потрібне повідомлення без втрат, і (2) має найменшу сумарну вартість. Це схоже на задачу
                    пошуку найкоротшого шляху в графі, де вершини — це стани вбудовування, а ребра — можливі модифікації
                    з їх вартостями.
                </p>
            </div>

            <p><strong>Як працює STC (концептуально):</strong></p>

            <div class="info-box">
                <p>
                    STC використовує математичну структуру під назвою <strong>trellis (граф-трелліс)</strong> — це
                    спеціальний тип графу з шарами. Уявіть багатоповерховий лабіринт, де кожен поверх представляє
                    один піксель зображення, а на кожному поверсі є кілька кімнат (станів). STC шукає оптимальний
                    шлях згори вниз через цей лабіринт.
                </p>
                <p style="margin-top: 1rem;">
                    Алгоритм базується на класичному <strong>алгоритмі Вітербі</strong> з теорії зв'язку, адаптованому
                    для стеганографії. Алгоритм працює швидко: для зображення з 250,000 пікселів він знаходить оптимальне
                    рішення за лічені секунди, тоді як перебір всіх варіантів зайняв би більше часу, ніж існує Всесвіт.
                </p>
                <p style="margin-top: 1rem;">
                    Ключова властивість STC: він гарантує, що <strong>отримане рішення є оптимальним</strong> — не існує
                    іншого способу вбудувати це повідомлення з меншою вартістю. Це робить комбінацію "адаптивна функція
                    вартості + STC" дуже потужною для створення важковиявної стеганографії.
                </p>
            </div>

            <div class="info-box">
                <h4>🎯 Що робить STC?</h4>
                <p>
                    STC знаходить <strong>оптимальну комбінацію змін</strong>, яка:
                </p>
                <ul>
                    <li>Вбудовує все повідомлення (без втрат)</li>
                    <li>Мінімізує сумарну вартість усіх змін</li>
                    <li>Робить це ефективно (за розумний час)</li>
                </ul>
            </div>

            <p><strong>Аналогія: Планування маршруту</strong></p>

            <p>
                Уявіть, що ви хочете проїхати з міста А до міста Б, маючи карту з вартостями доріг
                (деякі дороги дорогі, інші дешеві). STC — це GPS, який знаходить найдешевший маршрут,
                не просто обираючи найдешевші дороги, а знаходячи оптимальну комбінацію.
            </p>

            <h3>1.7. Алгоритм HUGO покроково</h3>

            <p>Тепер, коли ми зрозуміли всі компоненти, розглянемо, як HUGO працює від початку до кінця.</p>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Вхід: Зображення + Повідомлення</strong><br>
                        Маємо чисте зображення (наприклад, 512×512 пікселів) і повідомлення (наприклад, 1000 біт)
                    </li>
                    <li>
                        <strong>Обчислення SPAM для оригіналу</strong><br>
                        Аналізуємо все зображення і створюємо SPAM "відбиток" — 686 чисел
                    </li>
                    <li>
                        <strong>Обчислення вартості для кожного пікселя</strong><br>
                        Для кожного з 262,144 пікселів (512×512) обчислюємо, як зміна ±1 вплине на SPAM.
                        Це найдовший етап (~15 секунд)
                    </li>
                    <li>
                        <strong>STC кодування</strong><br>
                        Використовуємо STC для знаходження оптимального способу вбудувати 1000 біт з мінімальною сумарною вартістю
                    </li>
                    <li>
                        <strong>Модифікація зображення</strong><br>
                        Змінюємо обрані пікселі на ±1 згідно результату STC
                    </li>
                    <li>
                        <strong>Вихід: Стего-зображення</strong><br>
                        Отримуємо зображення з вбудованим повідомленням, яке виглядає ідентично оригіналу
                    </li>
                </ol>
            </div>

            <h3>1.8. Приклад: Карта вартості</h3>

            <p>
                Карта вартості — це візуалізація того, які пікселі "дорогі" для модифікації, а які "дешеві".
            </p>

            <div class="example-box">
                <h4>🗺️ Як виглядає карта вартості?</h4>
                <p>
                    Уявіть зображення пейзажу, де:
                </p>
                <ul>
                    <li><strong>Небо (верх):</strong> Яскраві білі області на карті = висока вартість = не вбудовувати</li>
                    <li><strong>Ліс (низ):</strong> Темні області на карті = низька вартість = вбудовувати тут</li>
                    <li><strong>Ребра об'єктів:</strong> Білі лінії = високі вартості = уникати</li>
                    <li><strong>Текстури:</strong> Темні плями = низькі вартості = ідеально для даних</li>
                </ul>
                <p>
                    HUGO автоматично вибере темні області (низька вартість) для вбудовування даних.
                </p>
            </div>

            <h3>1.9. Переваги HUGO</h3>

            <div class="column-card" style="background: #dcfce7; border-color: #10b981;">
                <h4>✅ Чому HUGO ефективний?</h4>
                <ul>
                    <li>
                        <strong>Висока безпека:</strong> Точність виявлення детекторами ~55-60% (майже випадковий вибір — 50%)
                    </li>
                    <li>
                        <strong>Теоретична обґрунтованість:</strong> Базується на моделюванні реальних детекторів,
                        а не на інтуїції
                    </li>
                    <li>
                        <strong>Автоматична адаптація:</strong> Не потребує ручного налаштування параметрів для різних зображень
                    </li>
                    <li>
                        <strong>Відмінна візуальна якість:</strong> Зміни непомітні людському оку (PSNR &gt; 50 dB)
                    </li>
                    <li>
                        <strong>Стійкість до різних детекторів:</strong> Працює проти SPAM, SPA, та інших статистичних детекторів
                    </li>
                </ul>
            </div>

            <h3>1.10. Недоліки та обмеження HUGO</h3>

            <div class="column-card" style="background: #fee2e2; border-color: #ef4444;">
                <h4>⚠️ Де HUGO програє?</h4>
                <ul>
                    <li>
                        <strong>Повільний:</strong> Обчислення вартості для 512×512 зображення займає ~15-20 секунд
                        (для порівняння: LSB — мілісекунди)
                    </li>
                    <li>
                        <strong>Високі вимоги до пам'яті:</strong> Потребує ~200 MB RAM для зберігання SPAM моделі
                    </li>
                    <li>
                        <strong>Тільки просторова область:</strong> Працює з BMP/PNG, але не з JPEG без модифікацій
                    </li>
                    <li>
                        <strong>Складність реалізації:</strong> Правильна імплементація STC і SPAM вимагає ретельності
                    </li>
                    <li>
                        <strong>Вразливість до CNN детекторів:</strong> Сучасні нейромережеві детектори показують
                        вищу точність (~70-75%)
                    </li>
                </ul>
            </div>

            <h3>1.11. HUGO vs інші методи</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Метод</th>
                        <th>Точність виявлення (0.4 bpp)</th>
                        <th>Швидкість (512×512)</th>
                        <th>Основна ідея</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LSB replacement</td>
                        <td>❌ 99% (легко виявити)</td>
                        <td>5 мс</td>
                        <td>Рівномірна заміна LSB</td>
                    </tr>
                    <tr>
                        <td><strong>HUGO</strong></td>
                        <td>✅ 68% (важко виявити)</td>
                        <td>18 сек</td>
                        <td>Моделювання SPAM детекторів</td>
                    </tr>
                    <tr>
                        <td>WOW</td>
                        <td>✅ 64% (краще HUGO)</td>
                        <td>3 сек</td>
                        <td>Напрямкові фільтри</td>
                    </tr>
                    <tr>
                        <td>S-UNIWARD</td>
                        <td>✅ 62% (найкраще)</td>
                        <td>5 сек</td>
                        <td>Універсальне відносне спотворення</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>Висновок:</strong> HUGO був піонером адаптивних методів і довів їх ефективність,
            хоча пізніші методи (WOW, S-UNIWARD) покращили його результати.</p>

            <h3>1.12. Стійкість до стегоаналізу</h3>

            <p>
                Головне питання для будь-якого стеганографічного методу: <strong>"Як добре він протистоїть виявленню?"</strong>
            </p>

            <div class="info-box">
                <h4>📊 Результати тестування HUGO</h4>
                <p><strong>База тестування:</strong> BOSSbase (10,000 зображень 512×512)</p>
                <p><strong>Детектор:</strong> SRM (Spatial Rich Model) з ensemble класифікатором</p>

                <table class="comparison-table" style="margin-top: 1rem;">
                    <thead>
                        <tr>
                            <th>Payload (bpp)</th>
                            <th>Точність виявлення</th>
                            <th>Інтерпретація</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0.1 bpp</td>
                            <td>53.2%</td>
                            <td>✅ Майже невиявний (50% = випадковий вибір)</td>
                        </tr>
                        <tr>
                            <td>0.2 bpp</td>
                            <td>60.2%</td>
                            <td>✅ Дуже добре</td>
                        </tr>
                        <tr>
                            <td>0.4 bpp</td>
                            <td>68.4%</td>
                            <td>⚠️ Помірно</td>
                        </tr>
                        <tr>
                            <td>0.6 bpp</td>
                            <td>76.8%</td>
                            <td>❌ Висока виявлюваність</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="warning-box">
                <h4>⚠️ Практична рекомендація</h4>
                <p>
                    Для максимальної безпеки використовуйте <strong>payload ≤ 0.3 bpp</strong>.
                    При вищих значеннях виявлюваність різко зростає.
                </p>
                <p>
                    <strong>Приклад:</strong> Для зображення 512×512 (262,144 пікселі):
                </p>
                <ul>
                    <li>0.1 bpp = ~26,000 біт ≈ 3.2 KB тексту</li>
                    <li>0.3 bpp = ~78,000 біт ≈ 9.8 KB тексту</li>
                    <li>0.4 bpp = ~105,000 біт ≈ 13 KB тексту</li>
                </ul>
            </div>

            <h3>1.13. Практичне застосування</h3>

            <p>Де і як використовується HUGO в реальному світі?</p>

            <div class="two-columns">
                <div class="column-card">
                    <h4>🔬 Дослідження та освіта</h4>
                    <ul>
                        <li>Benchmark для порівняння нових методів</li>
                        <li>Навчання студентів основам адаптивної стеганографії</li>
                        <li>Тестування нових детекторів</li>
                    </ul>
                </div>

                <div class="column-card">
                    <h4>🏢 Промислове застосування</h4>
                    <ul>
                        <li>Цифрові водяні знаки</li>
                        <li>Захист інтелектуальної власності</li>
                        <li>Автентифікація зображень</li>
                    </ul>
                </div>
            </div>

            <div class="important-box">
                <h4>⚖️ Етичні та правові аспекти</h4>
                <p>
                    HUGO, як і інші стеганографічні методи, є dual-use технологією. Використовуйте їх відповідально:
                </p>
                <ul>
                    <li><strong>Законне використання:</strong> Захист конфіденційності, водяні знаки, автентифікація</li>
                    <li><strong>Проблемне використання:</strong> Приховування незаконного контенту, обхід моніторингу</li>
                </ul>
                <p>
                    <strong>Пам'ятайте:</strong> Знання методів важливе для розуміння безпеки, але використання має бути етичним.
                </p>
            </div>

            <h3>1.14. Підсумки про HUGO</h3>

            <div class="info-box">
                <h4>🎯 Ключові моменти</h4>
                <ul>
                    <li>HUGO — <strong>піонер адаптивної стеганографії</strong>, який показав, що моделювання детекторів працює</li>
                    <li>Використовує <strong>SPAM модель</strong> для оцінки того, як детектори бачать зображення</li>
                    <li>Вводить поняття <strong>"вартості модифікації"</strong> для кожного пікселя</li>
                    <li>Використовує <strong>STC кодування</strong> для оптимального вбудовування</li>
                    <li><strong>Безпека:</strong> Точність виявлення ~60-70% при 0.2-0.4 bpp (дуже добре!)</li>
                    <li><strong>Недолік:</strong> Повільний (~18 сек для 512×512)</li>
                    <li><strong>Спадщина:</strong> Заклав основу для WOW та S-UNIWARD</li>
                </ul>
            </div>

        </section>

        <!-- WOW -->
        <section id="wow">
            <h2>Розділ 2: WOW - Wavelet Obtained Weights</h2>

            <div class="info-box" style="background: #f0fdf4; border-left: 5px solid #10b981; padding: 1.5rem; margin-bottom: 2rem;">
                <h4 style="color: #065f46; margin-bottom: 0.5rem;">📝 Розшифровка абревіатури</h4>
                <p style="margin: 0;"><strong>WOW</strong> = <strong>W</strong>avelet <strong>O</strong>btained <strong>W</strong>eights</p>
                <p style="margin: 0.5rem 0 0 0;"><strong>Український переклад:</strong> Вагові коефіцієнти, Отримані за допомогою Вейвлетів</p>
                <p style="margin: 0.5rem 0 0 0; font-style: italic; color: #065f46;">
                    Назва відображає ключовий механізм методу — використання вейвлет-перетворення для обчислення
                    вагових коефіцієнтів, які визначають, де можна безпечно вбудовувати дані. "Weights" (ваги) —
                    це міра придатності кожного пікселя для модифікації.
                </p>
            </div>

            <h3>2.1. Ідея WOW</h3>

            <p>
                WOW (Wavelet Obtained Weights) був розроблений у 2012 році V. Holub та J. Fridrich
                як відповідь на обмеження HUGO. Головне питання було: <strong>"Чи можна зробити це швидше
                і ефективніше?"</strong>
            </p>

            <div class="info-box">
                <h4>🎯 Центральна ідея WOW</h4>
                <p>
                    Замість аналізу глобальної статистики (як HUGO), WOW фокусується на <strong>локальній
                    структурі та напрямках</strong> в зображенні. Ключова інсайт: <strong>зміни вздовж
                    напрямків текстури більш помітні, ніж зміни перпендикулярно до них</strong>.
                </p>
            </div>

            <div class="example-box">
                <h4>🌊 Аналогія: Хвилі на воді</h4>
                <p>
                    Уявіть, що ви дивитеся на гладку поверхню води з паралельними хвилями. Якщо ви
                    кинете камінчик:
                </p>
                <ul>
                    <li><strong>Вздовж хвиль:</strong> Збурення буде дуже помітне — порушується паралельність</li>
                    <li><strong>Перпендикулярно до хвиль:</strong> Збурення менш помітне — просто ще одна хвиля</li>
                </ul>
                <p>
                    WOW робить те саме: знаходить "хвилі" (напрямки текстури) в зображенні і уникає змін вздовж них.
                </p>
            </div>

            <h3>2.2. Вейвлет-декомпозиція</h3>

            <p>
                <strong>Вейвлет-перетворення</strong> — це математичний інструмент, який розкладає зображення
                на компоненти різних масштабів і напрямків. Це схоже на те, як призма розкладає світло на кольори.
            </p>

            <div class="info-box">
                <h4>🌊 Теоретичні основи вейвлет-аналізу в WOW</h4>
                <p>
                    WOW використовує <strong>дискретне вейвлет-перетворення (DWT)</strong> для багаторівневого аналізу
                    текстури зображення. На відміну від HUGO, який аналізує глобальну статистику, WOW фокусується на
                    <strong>локальних напрямкових характеристиках</strong>.
                </p>
                <p style="margin-top: 1rem;">
                    Ключова філософська відмінність: HUGO питає "як детектор бачить зображення?", а WOW питає "де в
                    зображенні є структура?". WOW не намагається моделювати детектор — натомість він шукає <strong>напрямки
                    текстури</strong> і уникає змін вздовж цих напрямків.
                </p>
                <p style="margin-top: 1rem;">
                    Метод використовує спеціальні математичні фільтри (фільтри Daubechies), які дозволяють "розкласти"
                    зображення на компоненти, що показують структуру в різних напрямках. Уявіть, що ви дивитеся на
                    зображення через набір спеціальних окулярів: одні окуляри показують тільки горизонтальні лінії,
                    інші — тільки вертикальні, треті — діагональні. WOW аналізує всі ці "проекції" одночасно.
                </p>
            </div>

            <p><strong>Як вейвлет-перетворення розкладає зображення:</strong></p>

            <div class="info-box">
                <p>
                    Вейвлет-перетворення розбиває зображення на <strong>чотири частини</strong>:
                </p>
                <ul style="margin-top: 1rem; margin-left: 2rem;">
                    <li><strong>Загальна картина (LL):</strong> Зменшена копія зображення без дрібних деталей — показує,
                        що на зображенні загалом</li>
                    <li><strong>Горизонтальні деталі (LH):</strong> Виділяє вертикальні лінії та краї (наприклад, стовпи,
                        дерева, вертикальні межі об'єктів)</li>
                    <li><strong>Вертикальні деталі (HL):</strong> Виділяє горизонтальні лінії (наприклад, горизонт,
                        підлога, горизонтальні краї)</li>
                    <li><strong>Діагональні деталі (HH):</strong> Виділяє діагональні структури та текстуру (наприклад,
                        похилі дахи, складна текстура)</li>
                </ul>
                <p style="margin-top: 1rem;">
                    Ці чотири частини разом містять всю інформацію з оригінального зображення, але тепер інформація
                    <strong>організована за напрямками</strong>. Це дозволяє WOW побачити, в яких областях є сильна
                    напрямкова структура (яку небезпечно порушувати), а в яких — хаотична текстура (де можна вбудовувати).
                </p>
            </div>

            <p><strong>Що дає вейвлет-декомпозиція? (простими словами)</strong></p>

            <p>
                Вона "розбирає" зображення на частини:
            </p>

            <ul>
                <li><strong>Горизонтальні деталі:</strong> Вертикальні лінії, ребра (наприклад, стовпи, дерева)</li>
                <li><strong>Вертикальні деталі:</strong> Горизонтальні лінії (наприклад, горизонт, поверхні)</li>
                <li><strong>Діагональні деталі:</strong> Кути, діагональні структури</li>
                <li><strong>Загальна структура:</strong> Наближене зображення без деталей</li>
            </ul>

            <div class="info-box">
                <h4>🔍 Чому це корисно для стеганографії?</h4>
                <p>
                    Вейвлет-декомпозиція показує, де в зображенні є <strong>напрямкові структури</strong>.
                    Якщо ми знаємо, що в певній області є вертикальні лінії, ми уникнемо змін, які порушать ці лінії.
                </p>
            </div>

            <h3>2.3. Вагові коефіцієнти</h3>

            <p>
                Після вейвлет-декомпозиції WOW обчислює <strong>"вагові коефіцієнти"</strong> (звідси "Weights"
                у назві) для кожного пікселя.
            </p>

            <p><strong>Ваговий коефіцієнт — це міра "активності" текстури навколо пікселя.</strong></p>

            <div class="info-box">
                <h4>⚖️ Теорія обчислення вагових коефіцієнтів</h4>
                <p>
                    Центральна ідея WOW полягає в обчисленні <strong>напрямкових вагових коефіцієнтів</strong> на основі
                    вейвлет-субсмуг. Метод використовує принцип <strong>директивності</strong> (directionality): зміни вздовж
                    текстурних структур більш помітні, ніж зміни поперек них.
                </p>
                <p style="margin-top: 1rem;">
                    Процес обчислення можна пояснити так: для кожного пікселя WOW дивиться на його околицю (зазвичай
                    квадрат 3×3 або 5×5 пікселів) у <strong>трьох напрямкових проекціях</strong> (горизонтальній,
                    вертикальній, діагональній). У кожній проекції він вимірює "активність" — наскільки сильно змінюються
                    значення в цьому напрямку.
                </p>
                <p style="margin-top: 1rem;">
                    Наприклад, якщо піксель знаходиться на вертикальному краї (межа будинку), то:
                </p>
                <ul style="margin-left: 2rem;">
                    <li><strong>Вертикальна активність</strong> буде низькою (вздовж краю все однорідно)</li>
                    <li><strong>Горизонтальна активність</strong> буде високою (поперек краю є різкий перепад)</li>
                    <li><strong>Діагональна активність</strong> буде середньою</li>
                </ul>
                <p style="margin-top: 1rem;">
                    WOW комбінує ці три активності особливим чином: він використовує <strong>гармонічне середнє</strong>
                    (обернене до середнього обернених значень). Це математичний трюк, який забезпечує, що якщо хоча б в
                    одному напрямку активність низька (є структура), то загальна вартість буде високою. Таким чином,
                    WOW уникає модифікацій на структурованих ребрах, навіть якщо вони виглядають "активними" в інших напрямках.
                </p>
                <p style="margin-top: 1rem;">
                    Цей підхід виявився дуже ефективним на практиці: він швидший за HUGO (бо не потребує обчислення
                    686-вимірних SPAM ознак) і при цьому забезпечує порівнянну, а часто навіть кращу безпеку.
                </p>
            </div>

            <div class="two-columns">
                <div class="column-card">
                    <h4>🟦 Висока активність</h4>
                    <p><strong>Означає:</strong> Багато деталей, текстура, хаос</p>
                    <p><strong>Приклад:</strong> Ліс, трава, тканина</p>
                    <p><strong>Вага:</strong> Велика</p>
                    <p><strong>Вартість модифікації:</strong> Низька (можна міняти)</p>
                </div>

                <div class="column-card">
                    <h4>⬜ Низька активність</h4>
                    <p><strong>Означає:</strong> Гладка поверхня, мало деталей</p>
                    <p><strong>Приклад:</strong> Небо, стіна, вода</p>
                    <p><strong>Вага:</strong> Мала</p>
                    <p><strong>Вартість модифікації:</strong> Висока (не міняти!)</p>
                </div>
            </div>

            <p><strong>Ключова формула (концептуально):</strong></p>

            <div class="simple-formula">
                <p>Вартість модифікації = 1 / (Активність текстури + невелика константа)</p>
                <p><em>Чим вища активність, тим нижча вартість</em></p>
            </div>

            <h3>2.4. Алгоритм WOW покроково</h3>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Вхід: Зображення + Повідомлення</strong><br>
                        Маємо чисте зображення і повідомлення для вбудовування
                    </li>
                    <li>
                        <strong>Застосування вейвлет-фільтрів</strong><br>
                        Застосовуємо 3 напрямкові фільтри (горизонтальний, вертикальний, діагональний) до зображення.
                        Це дає 3 "карти активності"
                    </li>
                    <li>
                        <strong>Обчислення вагових коефіцієнтів</strong><br>
                        Для кожного пікселя сумуємо відгуки всіх трьох фільтрів. Це показує загальну активність текстури
                    </li>
                    <li>
                        <strong>Обчислення вартості</strong><br>
                        Вартість = 1 / (Вага + σ), де σ — невелика константа для стабільності
                    </li>
                    <li>
                        <strong>STC кодування</strong><br>
                        Використовуємо той же STC, що й HUGO, з обчисленими вартостями
                    </li>
                    <li>
                        <strong>Модифікація зображення</strong><br>
                        Змінюємо обрані пікселі на ±1
                    </li>
                    <li>
                        <strong>Вихід: Стего-зображення</strong><br>
                        Отримуємо зображення з вбудованим повідомленням
                    </li>
                </ol>
            </div>

            <h3>2.5. Чому WOW ефективний?</h3>

            <p>WOW має кілька причин успіху:</p>

            <div class="info-box">
                <h4>🎯 Переваги підходу WOW</h4>
                <ul>
                    <li>
                        <strong>Швидкість:</strong> Вейвлет-фільтри обчислюються набагато швидше, ніж повний
                        SPAM аналіз. WOW працює за ~3 секунди vs ~18 секунд у HUGO
                    </li>
                    <li>
                        <strong>Простота:</strong> Не потрібно обчислювати 686-вимірний SPAM вектор — тільки
                        3 відгуки фільтрів
                    </li>
                    <li>
                        <strong>Універсальність:</strong> Напрямкові фільтри "бачать" те, що бачать і детектори,
                        і людське око
                    </li>
                    <li>
                        <strong>Кращі результати:</strong> WOW показує точність виявлення ~64% vs ~68% у HUGO
                        (нижче = краще!)
                    </li>
                </ul>
            </div>

            <h3>2.6. Переваги WOW</h3>

            <div class="column-card" style="background: #dcfce7; border-color: #10b981;">
                <h4>✅ Сильні сторони WOW</h4>
                <ul>
                    <li><strong>Висока безпека:</strong> Точність виявлення ~64% при 0.4 bpp (краще за HUGO на 4%)</li>
                    <li><strong>Швидкість:</strong> 5-6× швидше за HUGO (3 сек vs 18 сек для 512×512)</li>
                    <li><strong>Менші вимоги до пам'яті:</strong> ~50 MB vs ~200 MB у HUGO</li>
                    <li><strong>Простіша реалізація:</strong> Не потрібно імплементувати повний SPAM</li>
                    <li><strong>Універсальність:</strong> Добре працює на різних типах зображень</li>
                    <li><strong>Візуальна якість:</strong> PSNR ~51 dB (відмінно)</li>
                </ul>
            </div>

            <h3>2.7. Недоліки та обмеження WOW</h3>

            <div class="column-card" style="background: #fee2e2; border-color: #ef4444;">
                <h4>⚠️ Слабкі сторони WOW</h4>
                <ul>
                    <li>
                        <strong>Залежність від фільтрів:</strong> Результати залежать від правильного вибору
                        вейвлет-базису (Daubechies 8 — стандартний вибір)
                    </li>
                    <li>
                        <strong>Параметр σ:</strong> Потребує підбору для оптимальних результатів
                        (зазвичай σ = 10⁻⁶, але може варіюватися)
                    </li>
                    <li>
                        <strong>Тільки просторова область:</strong> Як і HUGO, потребує модифікацій для роботи з JPEG
                    </li>
                    <li>
                        <strong>Можлива переадаптація:</strong> Якщо детектор "знає" про WOW фільтри, може спеціалізуватися
                    </li>
                </ul>
            </div>

            <h3>2.8. WOW vs HUGO</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Критерій</th>
                        <th>HUGO</th>
                        <th>WOW</th>
                        <th>Переможець</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Безпека (0.4 bpp)</strong></td>
                        <td>68.4% виявлення</td>
                        <td>64.1% виявлення</td>
                        <td>🏆 WOW</td>
                    </tr>
                    <tr>
                        <td><strong>Швидкість (512×512)</strong></td>
                        <td>18 секунд</td>
                        <td>3 секунди</td>
                        <td>🏆 WOW</td>
                    </tr>
                    <tr>
                        <td><strong>Пам'ять</strong></td>
                        <td>~200 MB</td>
                        <td>~50 MB</td>
                        <td>🏆 WOW</td>
                    </tr>
                    <tr>
                        <td><strong>Складність реалізації</strong></td>
                        <td>Висока (SPAM + STC)</td>
                        <td>Середня (фільтри + STC)</td>
                        <td>🏆 WOW</td>
                    </tr>
                    <tr>
                        <td><strong>Налаштування</strong></td>
                        <td>Мінімальне (автоматично)</td>
                        <td>Потребує підбору σ</td>
                        <td>🏆 HUGO</td>
                    </tr>
                    <tr>
                        <td><strong>Теоретична основа</strong></td>
                        <td>Моделювання детекторів</td>
                        <td>Напрямкові фільтри</td>
                        <td>⚖️ Різні підходи</td>
                    </tr>
                </tbody>
            </table>

            <div class="info-box">
                <h4>📊 Висновок порівняння</h4>
                <p>
                    WOW виграє за <strong>більшістю практичних критеріїв</strong>: швидше, безпечніше, менші вимоги.
                    HUGO має перевагу в теоретичній обґрунтованості та автоматичному налаштуванні.
                </p>
                <p>
                    <strong>Рекомендація:</strong> Для більшості практичних застосувань WOW — кращий вибір.
                </p>
            </div>

            <h3>2.9. Підсумки WOW</h3>

            <div class="info-box">
                <h4>🎯 Ключові моменти</h4>
                <ul>
                    <li>WOW використовує <strong>напрямкові вейвлет-фільтри</strong> для аналізу локальної структури</li>
                    <li>Головна ідея: <strong>зміни перпендикулярні до текстури менш помітні</strong>, ніж зміни вздовж неї</li>
                    <li>Вартість обчислюється через <strong>вагові коефіцієнти</strong> (активність текстури)</li>
                    <li><strong>Швидкість:</strong> ~3 секунди для 512×512 (6× швидше HUGO)</li>
                    <li><strong>Безпека:</strong> Точність виявлення ~64% при 0.4 bpp (краще HUGO)</li>
                    <li><strong>Практичність:</strong> Оптимальний баланс безпека/швидкість/простота</li>
                    <li><strong>Статус:</strong> Один з найпопулярніших методів для реальних застосувань</li>
                </ul>
            </div>

        </section>

        <!-- S-UNIWARD -->
        <section id="suniward">
            <h2>Розділ 3: S-UNIWARD - Spatial Universal Wavelet Relative Distortion</h2>

            <div class="info-box" style="background: #fef3c7; border-left: 5px solid #f59e0b; padding: 1.5rem; margin-bottom: 2rem;">
                <h4 style="color: #92400e; margin-bottom: 0.5rem;">📝 Розшифровка абревіатури</h4>
                <p style="margin: 0;"><strong>S-UNIWARD</strong> = <strong>S</strong>patial <strong>UNI</strong>versal <strong>WA</strong>velet <strong>R</strong>elative <strong>D</strong>istortion</p>
                <p style="margin: 0.5rem 0 0 0;"><strong>Український переклад:</strong></p>
                <ul style="margin: 0.5rem 0 0 1.5rem; padding-left: 0;">
                    <li><strong>S (Spatial)</strong> — Просторовий (працює з пікселями зображення)</li>
                    <li><strong>UNI (Universal)</strong> — Універсальний (підходить для різних форматів)</li>
                    <li><strong>WA (Wavelet)</strong> — Вейвлетний (використовує вейвлет-аналіз)</li>
                    <li><strong>R (Relative)</strong> — Відносне</li>
                    <li><strong>D (Distortion)</strong> — Спотворення</li>
                </ul>
                <p style="margin: 0.5rem 0 0 0; font-style: italic; color: #92400e;">
                    Повна назва: <strong>"Просторове Універсальне Вейвлетне Відносне Спотворення"</strong>. Кожне слово
                    підкреслює ключову властивість: метод працює в просторовій області (на відміну від частотної), є
                    універсальним (застосовується до різних типів контейнерів), використовує вейвлет-аналіз і базується
                    на концепції відносного (а не абсолютного) спотворення.
                </p>
            </div>

            <h3>3.1. Універсальність S-UNIWARD</h3>

            <p>
                S-UNIWARD (2014, V. Holub, J. Fridrich, T. Denemark) представляє <strong>третє покоління</strong>
                адаптивної стеганографії. Назва довга, але кожне слово важливе:
            </p>

            <ul>
                <li><strong>Spatial (Просторовий):</strong> Працює в просторовій області (з пікселями безпосередньо)</li>
                <li><strong>UNIversal (Універсальний):</strong> Працює в будь-якій області (spatial, JPEG, JPEG2000)</li>
                <li><strong>WAvelet (Вейвлетний):</strong> Використовує вейвлет-декомпозицію для аналізу</li>
                <li><strong>Relative Distortion (Відносне Спотворення):</strong> Базується на відносному (не абсолютному) спотворенні</li>
            </ul>

            <div class="info-box">
                <h4>🔬 Теоретичні основи універсальності</h4>
                <p>
                    S-UNIWARD базується на фундаментальній концепції <strong>універсальної функції спотворення</strong>,
                    яка може бути застосована до будь-якого представлення цифрового контейнера.
                </p>
                <p style="margin-top: 1rem;">
                    Ключова інновація полягає у використанні <strong>відносного спотворення</strong> замість абсолютного.
                    Щоб зрозуміти різницю, розглянемо аналогію зі звуком: якщо ви перебуваєте в тихій бібліотеці, шепіт
                    (абсолютно тихий звук) буде дуже помітним. Але на рок-концерті той самий шепіт буде непомітним на тлі
                    гучної музики. <strong>Відносне спотворення враховує цей контекст</strong> — воно вимірює зміну не
                    саму по собі, а відносно "шумності" оточення.
                </p>
                <p style="margin-top: 1rem;">
                    S-UNIWARD застосовує цю ідею до зображень: він вимірює, наскільки зміна пікселя помітна <strong>відносно
                    локальної активності</strong> навколо нього. У текстурованій області (як ліс) зміна на ±1 становить малий
                    відсоток від локальної варіації. У гладкій області (як небо) та сама зміна ±1 становить великий відсоток.
                </p>
                <p style="margin-top: 1rem;">
                    Чому це робить метод універсальним? Тому що <strong>принцип відносного спотворення працює однаково
                    в різних представленнях</strong>. Чи ми працюємо з пікселями, чи з DCT коефіцієнтами JPEG, чи з
                    вейвлет-коефіцієнтами JPEG2000 — завжди можна говорити про "відносну зміну в контексті локального оточення".
                    Це як універсальний адаптер, який підходить до будь-якої розетки.
                </p>
            </div>

            <div class="info-box">
                <h4>🌟 Головна інновація S-UNIWARD</h4>
                <p>
                    До S-UNIWARD кожен домен (spatial, DCT, JPEG) потребував окремого методу:
                </p>
                <ul>
                    <li>HUGO — тільки spatial (BMP/PNG)</li>
                    <li>WOW — тільки spatial</li>
                    <li>J-UNIWARD — тільки JPEG</li>
                </ul>
                <p>
                    <strong>S-UNIWARD вирішує це</strong>: одна функція вартості працює скрізь! Це як універсальний
                    адаптер, який підходить до будь-якої розетки.
                </p>
            </div>

            <h3>3.2. Багаторівнева вейвлет-декомпозиція</h3>

            <p>
                На відміну від WOW, який використовує тільки 1 рівень вейвлет-аналізу, S-UNIWARD використовує
                <strong>3 рівні багаторівневої декомпозиції</strong>.
            </p>

            <p><strong>Що це означає? (простими словами)</strong></p>

            <p>
                Уявіть, що ви дивитеся на зображення через три різні збільшувальні скла:
            </p>

            <ul>
                <li><strong>Рівень 1 (дрібні деталі):</strong> Бачите окремі листки на дереві, текстуру</li>
                <li><strong>Рівень 2 (середні деталі):</strong> Бачите гілки дерев, загальну структуру</li>
                <li><strong>Рівень 3 (великі деталі):</strong> Бачите цілі дерева, будинки</li>
            </ul>

            <p>
                S-UNIWARD аналізує зображення на всіх трьох рівнях одночасно. Це дає повнішу картину того,
                як зміни вплинуть на зображення.
            </p>

            <div class="example-box">
                <h4>🔍 Чому 3 рівні краще, ніж 1?</h4>
                <p>
                    Деякі структури видно тільки на певному масштабі:
                </p>
                <ul>
                    <li>Дрібна текстура (листя) — видно на рівні 1</li>
                    <li>Середні патерни (гілки) — видно на рівні 2</li>
                    <li>Загальна структура (силует дерева) — видно на рівні 3</li>
                </ul>
                <p>
                    Аналізуючи всі рівні, S-UNIWARD краще розуміє, де можна безпечно вбудувати дані.
                </p>
            </div>

            <h3>3.3. Обчислення відносного спотворення</h3>

            <p>
                Ключова концепція S-UNIWARD — <strong>відносне спотворення</strong>. Що це означає?
            </p>

            <div class="info-box">
                <h4>📐 Теорія відносного спотворення</h4>
                <p>
                    Відносне спотворення (relative distortion) — це нормалізована міра змін, яка враховує локальний контекст.
                    На відміну від абсолютних метрик, відносне спотворення масштабується згідно з локальною активністю сигналу.
                </p>
                <p style="margin-top: 1rem;">
                    Процес обчислення можна пояснити через три кроки:
                </p>
                <p style="margin-top: 1rem;">
                    <strong>Крок 1: Багаторівнева вейвлет-декомпозиція</strong><br>
                    S-UNIWARD розкладає зображення на 3 рівні деталізації. Кожен рівень показує структуру на певному масштабі:
                    перший рівень — дрібні деталі (текстура листя), другий — середні (гілки дерев), третій — великі (силуети
                    об'єктів). На кожному рівні є 3 напрямкові компоненти (горизонтальна, вертикальна, діагональна). Разом
                    це дає 9 "проекцій" зображення (3 рівні × 3 напрямки).
                </p>
                <p style="margin-top: 1rem;">
                    <strong>Крок 2: Обчислення локальної "енергії"</strong><br>
                    Для кожної з 9 проекцій S-UNIWARD обчислює локальну енергію навколо кожного пікселя — по суті, це міра
                    того, наскільки "активна" область. У текстурованих зонах енергія висока (багато змін), у гладких — низька.
                    Енергія обчислюється як сума абсолютних значень вейвлет-коефіцієнтів у локальній околиці.
                </p>
                <p style="margin-top: 1rem;">
                    <strong>Крок 3: Нормалізація спотворення на енергію</strong><br>
                    Коли піксель змінюється, це впливає на вейвлет-коефіцієнти навколо нього. S-UNIWARD вимірює це спотворення,
                    але <strong>ділить його на локальну енергію</strong>. Результат: в активних областях (висока енергія)
                    відносне спотворення мале, в гладких (низька енергія) — велике. Це автоматично спрямовує вбудовування
                    у текстуровані зони.
                </p>
                <p style="margin-top: 1rem;">
                    Фінальна вартість для пікселя — це сума відносних спотворень з усіх 9 проекцій. Така багаторівнева
                    багатонапрямкова агрегація дає дуже точну оцінку того, наскільки безпечно модифікувати конкретний піксель,
                    що і пояснює високу ефективність S-UNIWARD проти сучасних детекторів.
                </p>
            </div>

            <div class="two-columns">
                <div class="column-card">
                    <h4>❌ Абсолютне спотворення</h4>
                    <p><strong>Ідея:</strong> Міряємо просто величину зміни</p>
                    <p><strong>Приклад:</strong> Зміна з 100 до 101 = спотворення 1</p>
                    <p><strong>Проблема:</strong> Не враховує контекст!</p>
                    <p>Зміна з 1 до 2 теж дає спотворення 1, але це 100% зміна!</p>
                </div>

                <div class="column-card">
                    <h4>✅ Відносне спотворення</h4>
                    <p><strong>Ідея:</strong> Міряємо зміну відносно оточення</p>
                    <p><strong>Приклад:</strong> Зміна з 100 до 101 в області зі значеннями ~100 = малень дробове спотворення</p>
                    <p><strong>Перевага:</strong> Враховує контекст!</p>
                    <p>Зміна з 1 до 2 в області зі значеннями ~1 = велике відносне спотворення</p>
                </div>
            </div>

            <div class="simple-formula">
                <p><strong>Спрощена формула відносного спотворення:</strong></p>
                <p>Спотворення = Величина зміни / Локальна "енергія" сигналу</p>
                <p><em>Чим більша локальна активність, тим менше відносне спотворення</em></p>
            </div>

            <h3>3.4. Алгоритм S-UNIWARD</h3>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Вхід: Зображення + Повідомлення</strong><br>
                        Маємо чисте зображення і дані для вбудовування
                    </li>
                    <li>
                        <strong>3-рівнева вейвлет-декомпозиція</strong><br>
                        Застосовуємо DWT (Discrete Wavelet Transform) 3 рази. На кожному рівні отримуємо
                        3 субдіапазони (горизонтальний, вертикальний, діагональний)
                    </li>
                    <li>
                        <strong>Обчислення локальної енергії</strong><br>
                        Для кожного субдіапазону обчислюємо середнє абсолютних значень сусідніх коефіцієнтів.
                        Це показує локальну "активність"
                    </li>
                    <li>
                        <strong>Обчислення відносного спотворення</strong><br>
                        Для кожного пікселя обчислюємо, як зміна ±1 вплине на вейвлет-коефіцієнти відносно
                        локальної енергії
                    </li>
                    <li>
                        <strong>Агрегація по всіх рівнях</strong><br>
                        Сумуємо спотворення з усіх 3 рівнів і всіх 3 напрямків (всього 9 компонент).
                        Це дає загальну вартість модифікації
                    </li>
                    <li>
                        <strong>STC кодування</strong><br>
                        Використовуємо STC для оптимального вбудовування з обчисленими вартостями
                    </li>
                    <li>
                        <strong>Модифікація та вихід</strong><br>
                        Змінюємо пікселі і отримуємо стего-зображення
                    </li>
                </ol>
            </div>

            <h3>3.5. Чому "Universal"?</h3>

            <p>
                Універсальність S-UNIWARD означає, що <strong>та сама функція вартості працює в різних доменах</strong>.
            </p>

            <div class="info-box">
                <h4>🌍 Де працює S-UNIWARD?</h4>
                <ul>
                    <li><strong>Spatial domain (S-UNIWARD):</strong> Працює з пікселями BMP/PNG зображень</li>
                    <li><strong>JPEG domain (J-UNIWARD):</strong> Працює з DCT коефіцієнтами JPEG</li>
                    <li><strong>JPEG2000 domain:</strong> Працює з вейвлет-коефіцієнтами JPEG2000</li>
                </ul>
                <p>
                    <strong>Секрет:</strong> У всіх цих доменах є "коефіцієнти", і до них можна застосувати вейвлет-аналіз.
                    S-UNIWARD просто обчислює відносне спотворення в цих коефіцієнтах, незалежно від того, що вони означають.
                </p>
            </div>

            <h3>3.6. Багаторівнева декомпозиція детально</h3>

            <p>
                Розглянемо, що дає кожен рівень вейвлет-декомпозиції:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Рівень</th>
                        <th>Масштаб</th>
                        <th>Що виявляє</th>
                        <th>Приклад структур</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Рівень 1</strong></td>
                        <td>Дрібний (високі частоти)</td>
                        <td>Дрібні деталі, шум, текстура</td>
                        <td>Листя, зернистість, дрібні патерни</td>
                    </tr>
                    <tr>
                        <td><strong>Рівень 2</strong></td>
                        <td>Середній</td>
                        <td>Середні структури, патерни</td>
                        <td>Гілки, вікна, середні об'єкти</td>
                    </tr>
                    <tr>
                        <td><strong>Рівень 3</strong></td>
                        <td>Великий (низькі частоти)</td>
                        <td>Загальна структура, силуети</td>
                        <td>Будинки, дерева, великі об'єкти</td>
                    </tr>
                </tbody>
            </table>

            <p>
                <strong>Чому це важливо:</strong> Детектори стегоаналізу теж аналізують зображення на різних
                масштабах. S-UNIWARD "бачить" зображення так само, як і детектори, тому може краще мінімізувати
                виявлюваність.
            </p>

            <h3>3.7. Стійкість S-UNIWARD</h3>

            <div class="info-box">
                <h4>🛡️ Результати безпеки S-UNIWARD</h4>
                <p><strong>База тестування:</strong> BOSSbase (10,000 зображень 512×512)</p>

                <table class="comparison-table" style="margin-top: 1rem;">
                    <thead>
                        <tr>
                            <th>Payload (bpp)</th>
                            <th>HUGO</th>
                            <th>WOW</th>
                            <th>S-UNIWARD</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0.1 bpp</td>
                            <td>53.2%</td>
                            <td>51.8%</td>
                            <td>🏆 <strong>50.9%</strong></td>
                        </tr>
                        <tr>
                            <td>0.2 bpp</td>
                            <td>60.2%</td>
                            <td>58.5%</td>
                            <td>🏆 <strong>56.8%</strong></td>
                        </tr>
                        <tr>
                            <td>0.4 bpp</td>
                            <td>68.4%</td>
                            <td>64.1%</td>
                            <td>🏆 <strong>62.3%</strong></td>
                        </tr>
                        <tr>
                            <td>0.6 bpp</td>
                            <td>76.8%</td>
                            <td>71.5%</td>
                            <td>🏆 <strong>69.2%</strong></td>
                        </tr>
                    </tbody>
                </table>

                <p style="margin-top: 1rem;">
                    <strong>Висновок:</strong> S-UNIWARD показує <strong>найкращі результати</strong> при всіх
                    значеннях payload. При малому payload (0.1 bpp) точність виявлення майже випадкова (50.9% ≈ 50%).
                </p>
            </div>

            <h3>3.8. Переваги S-UNIWARD</h3>

            <div class="column-card" style="background: #dcfce7; border-color: #10b981;">
                <h4>✅ Сильні сторони S-UNIWARD</h4>
                <ul>
                    <li><strong>Найвища безпека:</strong> Кращі результати серед усіх spatial методів (~62% при 0.4 bpp)</li>
                    <li><strong>Універсальність:</strong> Працює в spatial, DCT, JPEG доменах</li>
                    <li><strong>Багаторівневий аналіз:</strong> 3 рівні декомпозиції дають повнішу картину</li>
                    <li><strong>Теоретична обґрунтованість:</strong> Базується на information-theoretic принципах</li>
                    <li><strong>Збалансована швидкість:</strong> ~5 сек для 512×512 (швидше HUGO, трохи повільніше WOW)</li>
                    <li><strong>Стійкість до сучасних детекторів:</strong> Навіть CNN детектори мають складнощі</li>
                    <li><strong>Автоматична адаптація:</strong> Мінімум параметрів для налаштування</li>
                </ul>
            </div>

            <h3>3.9. Підсумки S-UNIWARD</h3>

            <div class="info-box">
                <h4>🎯 Ключові моменти</h4>
                <ul>
                    <li>S-UNIWARD — <strong>поточний state-of-the-art</strong> адаптивної стеганографії</li>
                    <li>Використовує <strong>3-рівневу вейвлет-декомпозицію</strong> для багатомасштабного аналізу</li>
                    <li>Базується на концепції <strong>відносного спотворення</strong>, яка краще відображає перцептивну важливість</li>
                    <li><strong>Універсальність:</strong> Працює в різних доменах (spatial, JPEG, JPEG2000)</li>
                    <li><strong>Безпека:</strong> Точність виявлення ~62% при 0.4 bpp (найкраще!)</li>
                    <li><strong>Швидкість:</strong> ~5 секунд для 512×512 (прийнятно)</li>
                    <li><strong>Статус:</strong> Рекомендований вибір для production систем</li>
                </ul>
            </div>

        </section>

        <!-- Comparison -->
        <section id="comparison">
            <h2>Розділ 4: Порівняльний аналіз методів</h2>

            <h3>4.1. Три підходи до адаптивності</h3>

            <p>
                HUGO, WOW та S-UNIWARD вирішують одну задачу, але з різних точок зору. Розглянемо їх філософії:
            </p>

            <div class="info-box">
                <h4>🔬 Теоретична класифікація адаптивних методів</h4>
                <p>
                    Сучасні адаптивні методи стеганографії можна класифікувати за <strong>принципом формування функції вартості</strong> —
                    тобто, як саме кожен метод визначає, які пікселі безпечно змінювати:
                </p>
            </div>

            <p><strong>Три філософські підходи:</strong></p>

            <div class="info-box">
                <p style="margin-top: 1rem;">
                    <strong>1. Model-driven підхід (HUGO): "Моделюємо детектор"</strong><br>
                    HUGO базується на принципі <em>"знай свого ворога"</em>. Він аналізує, як працюють детектори стегоаналізу
                    (які ознаки вони обчислюють), і намагається мінімізувати зміни саме в цих ознаках. Це як гра в прятки,
                    де ви точно знаєте, куди дивиться шукач, і ховаєтесь саме у "сліпих зонах" його зору. Вартість пікселя —
                    це наскільки сильно його зміна вплине на SPAM-характеристики (686-вимірний статистичний відбиток), які
                    використовують детектори.
                </p>
                <p style="margin-top: 1rem;">
                    <strong>2. Filter-based підхід (WOW): "Аналізуємо структуру"</strong><br>
                    WOW базується на принципі <em>"шукай текстуру, уникай структури"</em>. Замість моделювання детекторів,
                    WOW аналізує саме зображення через напрямкові фільтри. Він шукає області, де є чітка структура (ребра,
                    лінії) і уникає їх, концентруючись на хаотичних текстурованих зонах. Вартість пікселя визначається через
                    гармонічне середнє трьох напрямкових активностей — якщо хоча б в одному напрямку є структура, вартість висока.
                </p>
                <p style="margin-top: 1rem;">
                    <strong>3. Universal distortion підхід (S-UNIWARD): "Відносне спотворення"</strong><br>
                    S-UNIWARD базується на принципі <em>"все відносно"</em>. Він вимірює зміни не абсолютно, а відносно
                    локального контексту через багаторівневу вейвлет-декомпозицію. Та сама зміна ±1 має різну "важливість"
                    в залежності від того, де вона відбувається: в шумній текстурі — мала важливість (низька вартість),
                    в гладкій області — велика важливість (висока вартість). Універсальність полягає у тому, що цей принцип
                    працює в будь-якому домені (пікселі, DCT, JPEG2000).
                </p>
            </div>

            <p style="margin-top: 1.5rem;"><strong>Спільна риса:</strong></p>

            <div class="info-box">
                <p>
                    Незалежно від філософії, всі три методи використовують <strong>STC (Syndrome Trellis Codes)</strong> для
                    фінального вбудовування. STC — це оптимізаційний алгоритм, який, знаючи вартість кожного пікселя, знаходить
                    найдешевший спосіб вбудувати повідомлення. Тобто різниця між методами — у тому, <em>як вони обчислюють вартості</em>,
                    а STC однаковий для всіх і використовує ці вартості для мінімізації загального спотворення.
                </p>
            </div>

            <div class="two-columns">
                <div class="column-card">
                    <h4>🎭 HUGO: Статистичний підхід</h4>
                    <p><strong>Філософія:</strong> "Моделюємо детектор"</p>
                    <p>
                        HUGO аналізує, як детектори стегоаналізу "бачать" зображення через SPAM модель,
                        і мінімізує зміни в цих ознаках.
                    </p>
                    <p><strong>Аналогія:</strong> Гра в прятки, де ми знаємо, як шукає шукач</p>
                </div>

                <div class="column-card">
                    <h4>🌊 WOW: Напрямковий підхід</h4>
                    <p><strong>Філософія:</strong> "Враховуємо структуру"</p>
                    <p>
                        WOW аналізує напрямки текстури в зображенні і уникає змін вздовж них,
                        використовуючи вейвлет-фільтри.
                    </p>
                    <p><strong>Аналогія:</strong> Ховаємось так, щоб не порушити паралельні лінії</p>
                </div>
            </div>

            <div class="column-card" style="margin-top: 1rem;">
                <h4>📐 S-UNIWARD: Багатомасштабний підхід</h4>
                <p><strong>Філософія:</strong> "Аналізуємо на всіх рівнях"</p>
                <p>
                    S-UNIWARD використовує 3-рівневу вейвлет-декомпозицію для оцінки відносного спотворення
                    на різних масштабах — від дрібних деталей до загальної структури.
                </p>
                <p><strong>Аналогія:</strong> Перевіряємо, чи помітна зміна при різному "збільшенні"</p>
            </div>

            <h3>4.2. Детальне порівняння</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Критерій</th>
                        <th>HUGO</th>
                        <th>WOW</th>
                        <th>S-UNIWARD</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Безпека (0.4 bpp)</strong></td>
                        <td>68.4%</td>
                        <td>64.1%</td>
                        <td>🏆 62.3%</td>
                    </tr>
                    <tr>
                        <td><strong>Швидкість (512×512)</strong></td>
                        <td>18 сек</td>
                        <td>🏆 3 сек</td>
                        <td>5 сек</td>
                    </tr>
                    <tr>
                        <td><strong>Пам'ять</strong></td>
                        <td>~200 MB</td>
                        <td>🏆 ~50 MB</td>
                        <td>~120 MB</td>
                    </tr>
                    <tr>
                        <td><strong>Складність реалізації</strong></td>
                        <td>Висока</td>
                        <td>🏆 Середня</td>
                        <td>Висока</td>
                    </tr>
                    <tr>
                        <td><strong>Універсальність</strong></td>
                        <td>Тільки spatial</td>
                        <td>Тільки spatial</td>
                        <td>🏆 Spatial + JPEG</td>
                    </tr>
                    <tr>
                        <td><strong>Візуальна якість (PSNR)</strong></td>
                        <td>~52 dB</td>
                        <td>~51 dB</td>
                        <td>🏆 ~53 dB</td>
                    </tr>
                    <tr>
                        <td><strong>Стійкість до CNN</strong></td>
                        <td>Добра</td>
                        <td>Дуже добра</td>
                        <td>🏆 Відмінна</td>
                    </tr>
                </tbody>
            </table>

            <h3>4.3. Стійкість до стегоаналізу</h3>

            <p>
                Найважливіший критерій — наскільки добре методи протистоять виявленню. Розглянемо результати
                проти різних детекторів:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Детектор</th>
                        <th>Тип</th>
                        <th>HUGO</th>
                        <th>WOW</th>
                        <th>S-UNIWARD</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Chi-square</td>
                        <td>Статистичний (старий)</td>
                        <td>50-52%</td>
                        <td>50-51%</td>
                        <td>50-51%</td>
                    </tr>
                    <tr>
                        <td>RS аналіз</td>
                        <td>Статистичний (старий)</td>
                        <td>51-53%</td>
                        <td>50-52%</td>
                        <td>50-51%</td>
                    </tr>
                    <tr>
                        <td>SPAM</td>
                        <td>Статистичний (ML)</td>
                        <td>65-70%</td>
                        <td>60-65%</td>
                        <td>58-62%</td>
                    </tr>
                    <tr>
                        <td>SRM</td>
                        <td>Статистичний (ML)</td>
                        <td>68%</td>
                        <td>64%</td>
                        <td>🏆 62%</td>
                    </tr>
                    <tr>
                        <td>Xu-Net</td>
                        <td>CNN (5 шарів)</td>
                        <td>75%</td>
                        <td>72%</td>
                        <td>🏆 70%</td>
                    </tr>
                    <tr>
                        <td>SRNet</td>
                        <td>CNN (12 шарів)</td>
                        <td>78%</td>
                        <td>75%</td>
                        <td>🏆 72%</td>
                    </tr>
                </tbody>
            </table>

            <p>
                <strong>Висновок:</strong> S-UNIWARD найстійкіший до всіх типів детекторів. Різниця особливо
                помітна проти сучасних CNN детекторів.
            </p>

            <h3>4.4. Порівняння швидкодії</h3>

            <p>
                Швидкість роботи критична для практичних застосувань. Розглянемо, скільки часу займає кожен етап:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Етап</th>
                        <th>HUGO</th>
                        <th>WOW</th>
                        <th>S-UNIWARD</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Аналіз зображення</td>
                        <td>~50 мс (SPAM)</td>
                        <td>~10 мс (фільтри)</td>
                        <td>~30 мс (DWT)</td>
                    </tr>
                    <tr>
                        <td>Обчислення вартостей</td>
                        <td>~15 сек</td>
                        <td>~2 сек</td>
                        <td>~3 сек</td>
                    </tr>
                    <tr>
                        <td>STC кодування</td>
                        <td>~2 сек</td>
                        <td>~1 сек</td>
                        <td>~1.5 сек</td>
                    </tr>
                    <tr>
                        <td>Модифікація</td>
                        <td>&lt;1 мс</td>
                        <td>&lt;1 мс</td>
                        <td>&lt;1 мс</td>
                    </tr>
                    <tr>
                        <td><strong>Загалом (512×512)</strong></td>
                        <td><strong>~18 сек</strong></td>
                        <td><strong>🏆 ~3 сек</strong></td>
                        <td><strong>~5 сек</strong></td>
                    </tr>
                </tbody>
            </table>

            <p>
                <strong>Bottleneck:</strong> Для всіх методів найдовший етап — обчислення вартостей для кожного пікселя.
                WOW найшвидший, бо використовує прості фільтри замість складних моделей.
            </p>

            <h3>4.5. Коли використовувати який метод?</h3>

            <div class="info-box">
                <h4>🎯 Практичні рекомендації</h4>

                <p><strong>Вибирайте HUGO, якщо:</strong></p>
                <ul>
                    <li>Вам потрібна стійкість до специфічних SPAM/SPA детекторів</li>
                    <li>Швидкість не критична (можна чекати 15-20 секунд)</li>
                    <li>Ви працюєте в академічному контексті (HUGO — класика)</li>
                    <li>Важлива "історична" перевірена технологія</li>
                </ul>

                <p><strong>Вибирайте WOW, якщо:</strong></p>
                <ul>
                    <li>Пріоритет — <strong>швидкість</strong> (real-time або batch processing)</li>
                    <li>Обмежені ресурси (мобільні пристрої, embedded системи)</li>
                    <li>Потрібен баланс безпека/швидкість</li>
                    <li>Простота реалізації критична</li>
                    <li>Обробляєте велику кількість зображень</li>
                </ul>

                <p><strong>Вибирайте S-UNIWARD, якщо:</strong></p>
                <ul>
                    <li>Потрібна <strong>максимальна безпека</strong> (найкращі показники)</li>
                    <li>Працюєте з різними форматами (BMP, JPEG, можливо відео)</li>
                    <li>Це production система (S-UNIWARD — поточний SOTA)</li>
                    <li>Потрібна стійкість до сучасних CNN детекторів</li>
                    <li>Можете пожертвувати трохи швидкості заради безпеки</li>
                </ul>
            </div>

            <h3>4.6. Еволюція адаптивних методів</h3>

            <p>
                Розглянемо, як адаптивні методи розвивалися з часом:
            </p>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>2010: HUGO</strong><br>
                        Перший успішний адаптивний метод. Довів, що моделювання детекторів працює.
                        Точність виявлення впала з 99% (LSB) до 68% (HUGO).
                    </li>
                    <li>
                        <strong>2012: WOW</strong><br>
                        Покращив HUGO, зробивши метод швидшим (6×) і безпечнішим (64% vs 68%).
                        Показав, що напрямковий підхід ефективніший за статистичний.
                    </li>
                    <li>
                        <strong>2014: S-UNIWARD</strong><br>
                        Поточний state-of-the-art. Найбезпечніший (62%) і універсальний.
                        Багаторівнева декомпозиція дала додаткові 2% покращення.
                    </li>
                    <li>
                        <strong>2015-2020: CNN детектори</strong><br>
                        З'явилися нейромережеві детектори (Xu-Net, SRNet, YedroudNet), які підняли точність
                        виявлення до 72-88%. Це стимулювало нові дослідження.
                    </li>
                    <li>
                        <strong>2020-теперішній час: GAN-based методи</strong><br>
                        З'являються методи на основі GAN (Generative Adversarial Networks), які "вчаться"
                        протистояти детекторам. Але вони все ще експериментальні.
                    </li>
                </ol>
            </div>

            <h3>4.7. Сучасні тренди та майбутнє</h3>

            <div class="two-columns">
                <div class="column-card">
                    <h4>🔬 Активні напрямки досліджень</h4>
                    <ul>
                        <li><strong>Adversarial training:</strong> Навчання методів проти CNN детекторів</li>
                        <li><strong>GAN стеганографія:</strong> Генерація контейнерів замість модифікації</li>
                        <li><strong>Coverless стеганографія:</strong> Приховування без змін взагалі</li>
                        <li><strong>Відео стеганографія:</strong> Адаптивні методи для відео</li>
                    </ul>
                </div>

                <div class="column-card">
                    <h4>⚙️ Практичні виклики</h4>
                    <ul>
                        <li><strong>Real-time вбудовування:</strong> Потрібно &lt;1 сек, зараз ~3-5 сек</li>
                        <li><strong>Мобільні імплементації:</strong> Оптимізація для обмежених ресурсів</li>
                        <li><strong>Стійкість до post-processing:</strong> Стиснення, фільтри руйнують дані</li>
                        <li><strong>Стандартизація:</strong> Немає єдиних стандартів</li>
                    </ul>
                </div>
            </div>

            <h3>4.8. Практичні поради</h3>

            <div class="warning-box">
                <h4>⚠️ Що потрібно пам'ятати при практичному застосуванні</h4>
                <ul>
                    <li>
                        <strong>Обмежте payload:</strong> Не перевищуйте 0.3-0.4 bpp. Краще використати кілька
                        зображень, ніж одне з високим payload.
                    </li>
                    <li>
                        <strong>Вибір контейнера:</strong> Використовуйте текстуровані зображення високої якості
                        (≥1 Мпікс). Уникайте гладких, низькоякісних або стиснутих зображень.
                    </li>
                    <li>
                        <strong>Формат файлу:</strong> Для максимальної безпеки використовуйте lossless формати
                        (PNG, BMP). JPEG вимагає спеціалізованих методів (J-UNIWARD).
                    </li>
                    <li>
                        <strong>Тестування:</strong> Перевіряйте свої стегоконтейнери публічними детекторами
                        (Aletheia, StegExpose) перед використанням.
                    </li>
                    <li>
                        <strong>Диверсифікація:</strong> Використовуйте різні методи для різних зображень.
                        Якщо противник знає конкретний метод, він може оптимізувати детектор.
                    </li>
                    <li>
                        <strong>Оновлення:</strong> Слідкуйте за новими дослідженнями. Методи, безпечні сьогодні,
                        можуть стати вразливими завтра з появою нових детекторів.
                    </li>
                </ul>
            </div>

            <h3>4.9. Загальні підсумки</h3>

            <div class="info-box">
                <h4>🎯 Головні висновки</h4>
                <ul>
                    <li>
                        <strong>Адаптивні методи революціонізували стеганографію:</strong> Точність виявлення
                        впала з 99% (LSB) до 62% (S-UNIWARD) — майже до випадкового вибору (50%).
                    </li>
                    <li>
                        <strong>Три покоління, три підходи:</strong> HUGO (статистичний), WOW (напрямковий),
                        S-UNIWARD (багатомасштабний) — кожен покращує попередній.
                    </li>
                    <li>
                        <strong>Немає "ідеального" методу:</strong> S-UNIWARD найбезпечніший, WOW найшвидший,
                        HUGO найпростіший у налаштуванні. Вибір залежить від завдання.
                    </li>
                    <li>
                        <strong>Payload критичний:</strong> При ≤0.3 bpp методи майже невиявні. При &gt;0.5 bpp
                        виявлюваність різко зростає.
                    </li>
                    <li>
                        <strong>CNN детектори — нова загроза:</strong> Нейромережеві детектори показують на 5-10%
                        вищу точність. Це стимулює нові дослідження.
                    </li>
                    <li>
                        <strong>Майбутнє — за GAN та adversarial training:</strong> Наступне покоління методів
                        буде "вчитися" протистояти детекторам.
                    </li>
                </ul>
            </div>

        </section>

        <!-- Висновки -->
        <section id="conclusion">
            <h2>Висновки</h2>

            <p>
                Адаптивні методи стеганографії — HUGO, WOW та S-UNIWARD — представляють собою сучасний
                state-of-the-art у галузі приховування інформації. Вони фундаментально відрізняються від
                класичних методів своєю здатністю "розуміти" зображення та адаптувати вбудовування.
            </p>

            <h3>Ключові висновки</h3>

            <div class="two-columns">
                <div class="column-card">
                    <h4>📚 Що ми дізналися</h4>
                    <ul>
                        <li>Адаптивна стеганографія — це аналіз локальних властивостей контейнера</li>
                        <li>HUGO моделює детектори через SPAM</li>
                        <li>WOW використовує напрямкові фільтри</li>
                        <li>S-UNIWARD аналізує 3 масштаби одночасно</li>
                        <li>Точність виявлення ~60-70% (vs 99% для LSB)</li>
                    </ul>
                </div>

                <div class="column-card">
                    <h4>🎯 Що важливо пам'ятати</h4>
                    <ul>
                        <li>Payload ≤ 0.3 bpp для безпеки</li>
                        <li>Вибір методу залежить від завдання</li>
                        <li>Текстуровані зображення краще гладких</li>
                        <li>Тестуйте перед використанням</li>
                        <li>Слідкуйте за новими дослідженнями</li>
                    </ul>
                </div>
            </div>

            <div class="important-box">
                <h4>⚖️ Фінальна думка</h4>
                <p>
                    Адаптивна стеганографія — це не "остаточне рішення", а етап у безперервному змаганні
                    між методами приховування та детектування. Успіх залежить від:
                </p>
                <ul>
                    <li><strong>Розуміння принципів:</strong> Чому метод працює, а не тільки як</li>
                    <li><strong>Правильного вибору:</strong> Який метод підходить для вашого завдання</li>
                    <li><strong>Обережного застосування:</strong> Тестування, обмеження payload, вибір контейнера</li>
                    <li><strong>Етичності:</strong> Використання для законних цілей</li>
                </ul>
                <p>
                    <strong>Пам'ятайте:</strong> Ці методи — потужні інструменти. Використовуйте їх відповідально
                    для захисту конфіденційності, а не для незаконних дій.
                </p>
            </div>

        </section>

        <!-- Practical Examples -->
        <section id="examples">
            <h2>Додаток 1: Практичні приклади</h2>

            <h3>Приклад 1: Вибір зображення для вбудовування</h3>

            <p>
                Розглянемо три різні зображення і оцінимо їх придатність для адаптивної стеганографії.
            </p>

            <div class="two-columns">
                <div class="column-card">
                    <h4>❌ Погане зображення</h4>
                    <p><strong>Опис:</strong> Фото блакитного неба без хмар</p>
                    <ul>
                        <li>95% пікселів мають значення 145-155</li>
                        <li>Дуже низька складність</li>
                        <li>Майже всі пікселі мають високу вартість</li>
                    </ul>
                    <p><strong>Результат:</strong> Можна вбудувати ~0.05 bpp безпечно (дуже мало!)</p>
                </div>

                <div class="column-card">
                    <h4>✅ Гарне зображення</h4>
                    <p><strong>Опис:</strong> Фото лісу з деревами та листям</p>
                    <ul>
                        <li>Пікселі мають широкий діапазон 50-200</li>
                        <li>Висока складність і текстура</li>
                        <li>60-70% пікселів мають низьку вартість</li>
                    </ul>
                    <p><strong>Результат:</strong> Можна безпечно вбудувати 0.3-0.4 bpp</p>
                </div>
            </div>

            <div class="column-card" style="margin-top: 1rem;">
                <h4>⭐ Ідеальне зображення</h4>
                <p><strong>Опис:</strong> Фото міського пейзажу з будинками, вікнами, деревами, людьми</p>
                <ul>
                    <li>Різноманітні текстури: гладкі стіни + детальні вікна + зелень</li>
                    <li>Природний баланс простих і складних областей</li>
                    <li>70-80% пікселів придатні для вбудовування</li>
                </ul>
                <p><strong>Результат:</strong> Можна вбудувати 0.4-0.5 bpp з високою безпекою</p>
            </div>

            <h3>Приклад 2: Покрокове вбудовування з WOW</h3>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Маємо:</strong> Зображення природи 1024×768 = 786,432 пікселі<br>
                        <strong>Хочемо вбудувати:</strong> Секретне повідомлення "Зустріч о 15:00 біля фонтану" = 288 біт
                    </li>
                    <li>
                        <strong>Обчислюємо payload:</strong><br>
                        288 біт / 786,432 пікселі ≈ 0.00037 bpp (дуже мало, дуже безпечно!)
                    </li>
                    <li>
                        <strong>WOW аналізує зображення:</strong><br>
                        Застосовує 3 вейвлет-фільтри, обчислює вагові коефіцієнти (~2 секунди)
                    </li>
                    <li>
                        <strong>Отримуємо карту вартості:</strong><br>
                        - Небо (верх): вартість 0.8-1.2 (висока)<br>
                        - Ліс (низ): вартість 0.01-0.05 (низька)<br>
                        - Трава: вартість 0.02-0.08 (низька)
                    </li>
                    <li>
                        <strong>STC вибирає ~300 найдешевших пікселів</strong><br>
                        Всі пікселі в лісі та траві, жодного в небі
                    </li>
                    <li>
                        <strong>Модифікація:</strong><br>
                        300 пікселів змінено на ±1. Візуально різниця непомітна
                    </li>
                    <li>
                        <strong>Результат:</strong><br>
                        PSNR = 56 dB (відмінно!), точність виявлення ≈ 51% (майже невиявний!)
                    </li>
                </ol>
            </div>

            <h3>Приклад 3: Порівняння методів на одному зображенні</h3>

            <p>
                Візьмемо одне зображення (512×512, фото міста) і вбудуємо 10,000 біт (≈0.038 bpp) трьома методами.
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Метод</th>
                        <th>Час вбудовування</th>
                        <th>Змінено пікселів</th>
                        <th>PSNR</th>
                        <th>Точність виявлення</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LSB (для порівняння)</td>
                        <td>&lt;1 мс</td>
                        <td>10,000 (випадкові)</td>
                        <td>51 dB</td>
                        <td>❌ 89% (легко виявити)</td>
                    </tr>
                    <tr>
                        <td>HUGO</td>
                        <td>16 сек</td>
                        <td>~8,500 (складні області)</td>
                        <td>54 dB</td>
                        <td>✅ 56% (добре!)</td>
                    </tr>
                    <tr>
                        <td>WOW</td>
                        <td>2.8 сек</td>
                        <td>~8,200 (текстуровані)</td>
                        <td>55 dB</td>
                        <td>✅ 54% (краще!)</td>
                    </tr>
                    <tr>
                        <td>S-UNIWARD</td>
                        <td>4.5 сек</td>
                        <td>~7,900 (оптимальні)</td>
                        <td>56 dB</td>
                        <td>✅ 52% (найкраще!)</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>Висновок з прикладу:</strong> При малому payload всі адаптивні методи показують чудові результати. S-UNIWARD трохи кращий, але різниця невелика. WOW найшвидший — хороший вибір для практики.</p>

        </section>

        <!-- Real-World Scenarios -->
        <section id="scenarios">
            <h2>Додаток 2: Реальні сценарії використання</h2>

            <h3>Сценарій 1: Захист авторських прав на фотографії</h3>

            <div class="example-box">
                <h4>📸 Завдання фотографа</h4>
                <p>
                    Професійний фотограф публікує свої роботи онлайн і хоче вбудувати невидимий водяний знак
                    з інформацією про авторство.
                </p>

                <p><strong>Вимоги:</strong></p>
                <ul>
                    <li>Водяний знак має бути невидимий (PSNR &gt; 50 dB)</li>
                    <li>Стійкий до стиснення JPEG (якість 85-95%)</li>
                    <li>Містить: ім'я автора + дата + унікальний ID (всього ~200 біт)</li>
                    <li>Швидкість обробки (тисячі фото)</li>
                </ul>

                <p><strong>Рішення:</strong></p>
                <div class="step-list">
                    <ol>
                        <li><strong>Вибір методу:</strong> S-UNIWARD (найстійкіший до стиснення)</li>
                        <li><strong>Payload:</strong> 200 біт / (1920×1080) ≈ 0.0001 bpp (мізерно мало!)</li>
                        <li><strong>Обробка:</strong> ~6 секунд на фото на звичайному ПК</li>
                        <li><strong>Результат:</strong> Водяний знак невидимий, зберігається навіть після JPEG (q=90)</li>
                    </ol>
                </div>
            </div>

            <h3>Сценарій 2: Конфіденційна комунікація</h3>

            <div class="example-box">
                <h4>🔐 Завдання журналіста</h4>
                <p>
                    Журналіст-розслідувач працює над чутливою темою і хоче безпечно передати інформацію
                    редакторові через публічні канали (соцмережі).
                </p>

                <p><strong>Вимоги:</strong></p>
                <ul>
                    <li>Максимальна безпека (протистояти державним агентствам)</li>
                    <li>Використання звичайних фото (щоб не привертати увагу)</li>
                    <li>Повідомлення: адреса + час + короткий опис (~500 біт)</li>
                </ul>

                <p><strong>Рішення:</strong></p>
                <div class="step-list">
                    <ol>
                        <li><strong>Підготовка:</strong> Вибір 3 різних фото туристичних місць (природні, містяться багато деталей)</li>
                        <li><strong>Розподіл:</strong> 500 біт / 3 фото ≈ 167 біт на фото</li>
                        <li><strong>Метод:</strong> S-UNIWARD з payload &lt; 0.001 bpp (надбезпечно)</li>
                        <li><strong>Додатковий захист:</strong> Попереднє шифрування повідомлення AES-256</li>
                        <li><strong>Публікація:</strong> Фото публікуються з інтервалом як звичайні пости</li>
                        <li><strong>Результат:</strong> Точність виявлення навіть найкращими детекторами ~51% (практично невиявний)</li>
                    </ol>
                </div>
            </div>

            <h3>Сценарій 3: Автентифікація медичних зображень</h3>

            <div class="example-box">
                <h4>🏥 Завдання лікарні</h4>
                <p>
                    Лікарня хоче вбудовувати метадані пацієнта (ID, дата, лікар) в медичні зображення (рентген, МРТ)
                    для запобігання підробок і забезпечення цілісності.
                </p>

                <p><strong>Вимоги:</strong></p>
                <ul>
                    <li>Абсолютно невидимі зміни (діагностична цінність не може бути порушена)</li>
                    <li>Дані: ID пацієнта + timestamp + хеш (256 біт)</li>
                    <li>Автоматична обробка при створенні зображення</li>
                </ul>

                <p><strong>Рішення:</strong></p>
                <div class="step-list">
                    <ol>
                        <li><strong>Аналіз зображень:</strong> Медичні зображення часто мають високу складність (тканини, структури)</li>
                        <li><strong>Вибір методу:</strong> WOW (швидший, достатньо безпечний для цього застосування)</li>
                        <li><strong>Payload:</strong> 256 біт / (1024×1024) ≈ 0.00024 bpp</li>
                        <li><strong>Інтеграція:</strong> Автоматичне вбудовування при експорті з апарату МРТ</li>
                        <li><strong>Верифікація:</strong> Лікар може витягти і перевірити метадані перед діагнозом</li>
                        <li><strong>Результат:</strong> PSNR &gt; 55 dB, діагностична якість збережена, автентичність гарантована</li>
                    </ol>
                </div>
            </div>

        </section>

        <!-- Common Mistakes -->
        <section id="mistakes">
            <h2>Додаток 3: Поширені помилки та як їх уникнути</h2>

            <h3>Помилка 1: Надто високий payload</h3>

            <div class="warning-box">
                <h4>❌ Що роблять неправильно</h4>
                <p>
                    Студент хоче вбудувати великий текстовий файл (50 KB) в одне зображення 512×512,
                    отримуючи payload ≈ 1.5 bpp.
                </p>
                <p><strong>Результат:</strong> Точність виявлення ~85-90% (легко виявити навіть з S-UNIWARD)</p>
            </div>

            <div class="info-box">
                <h4>✅ Правильний підхід</h4>
                <p>Розділити дані на кілька зображень:</p>
                <ul>
                    <li>50 KB = 400,000 біт</li>
                    <li>Використати 5 зображень 512×512</li>
                    <li>Кожне: 80,000 біт / 262,144 ≈ 0.3 bpp</li>
                    <li><strong>Результат:</strong> Точність виявлення ~62-65% (прийнятно)</li>
                </ul>
            </div>

            <h3>Помилка 2: Невдалий вибір зображення</h3>

            <div class="warning-box">
                <h4>❌ Погані приклади</h4>
                <ul>
                    <li><strong>Скріншоти з комп'ютера:</strong> Великі одноколірні області (фон робочого столу)</li>
                    <li><strong>Графіка/малюнки:</strong> Штучні, ідеально рівні кольори</li>
                    <li><strong>Низькоякісні фото:</strong> Вже стиснуті JPEG з артефактами</li>
                    <li><strong>Чорно-білі документи:</strong> Тільки 2 кольори, немає градієнтів</li>
                </ul>
            </div>

            <div class="info-box">
                <h4>✅ Хороші приклади</h4>
                <ul>
                    <li><strong>Природні фото:</strong> Ліс, трава, камені — багато текстури</li>
                    <li><strong>Міські пейзажі:</strong> Будинки, вікна, вулиці — різноманітні структури</li>
                    <li><strong>Портрети на вулиці:</strong> Людина + фон з деталями</li>
                    <li><strong>Фото їжі:</strong> Різні текстури тарілок, їжі, фону</li>
                </ul>
            </div>

            <h3>Помилка 3: Неправильний формат файлу</h3>

            <div class="warning-box">
                <h4>❌ Що роблять неправильно</h4>
                <p>
                    Вбудовують дані методом S-UNIWARD (spatial) в PNG, потім зберігають як JPEG для
                    зменшення розміру файлу.
                </p>
                <p><strong>Результат:</strong> JPEG стиснення руйнує вбудовані дані — повідомлення втрачено!</p>
            </div>

            <div class="info-box">
                <h4>✅ Правильний підхід</h4>
                <p><strong>Варіант 1:</strong> Використовувати lossless формати</p>
                <ul>
                    <li>Вбудувати в PNG, зберегти як PNG (або BMP)</li>
                    <li>Гарантія: дані не пошкоджено</li>
                    <li>Недолік: великий розмір файлу</li>
                </ul>

                <p><strong>Варіант 2:</strong> Використовувати J-UNIWARD для JPEG</p>
                <ul>
                    <li>Вбудувати безпосередньо в JPEG DCT коефіцієнти</li>
                    <li>Зберегти як JPEG</li>
                    <li>Гарантія: дані стійкі до JPEG compression</li>
                </ul>
            </div>

            <h3>Помилка 4: Ігнорування тестування</h3>

            <div class="warning-box">
                <h4>❌ Небезпечна практика</h4>
                <p>
                    Студент реалізує метод, вбудовує конфіденційні дані і відправляє зображення, не перевіривши
                    його детекторами.
                </p>
                <p><strong>Ризик:</strong> Якщо в реалізації є помилка, дані можуть бути легко виявлені!</p>
            </div>

            <div class="info-box">
                <h4>✅ Правильна процедура</h4>
                <div class="step-list">
                    <ol>
                        <li><strong>Імплементація:</strong> Реалізуйте метод</li>
                        <li><strong>Unit тести:</strong> Перевірте вбудовування/витягування на тестових даних</li>
                        <li><strong>Візуальна перевірка:</strong> Переконайтеся, що PSNR &gt; 45 dB</li>
                        <li><strong>Детектори:</strong> Протестуйте публічними детекторами (Aletheia, StegExpose)</li>
                        <li><strong>Benchmark:</strong> Порівняйте з reference implementation на BOSSbase</li>
                        <li><strong>Тільки після цього:</strong> Використовуйте для реальних даних</li>
                    </ol>
                </div>
            </div>

            <h3>Помилка 5: Повторне використання контейнерів</h3>

            <div class="warning-box">
                <h4>❌ Дуже небезпечно!</h4>
                <p>
                    Використовувати одне зображення кілька разів для вбудовування різних повідомлень.
                </p>
                <p><strong>Проблема:</strong> Якщо противник має оригінал і стегоконтейнер, він може порівняти і знайти точні зміни!</p>
            </div>

            <div class="info-box">
                <h4>✅ Золоте правило</h4>
                <p><strong>Один контейнер = одне повідомлення = одна передача</strong></p>
                <p>Кожне нове повідомлення вимагає нового, не використаного раніше зображення.</p>
            </div>

        </section>

        <!-- Tips and Tricks -->
        <section id="tips">
            <h2>Додаток 4: Поради та хитрощі</h2>

            <h3>Порада 1: Оптимізація вибору контейнера</h3>

            <div class="info-box">
                <h4>🎯 Критерії ідеального контейнера</h4>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Критерій</th>
                            <th>Бажане значення</th>
                            <th>Як перевірити</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Розмір</td>
                            <td>≥ 512×512 пікселів</td>
                            <td>Властивості файлу</td>
                        </tr>
                        <tr>
                            <td>Текстурованість</td>
                            <td>Багато деталей і варіації</td>
                            <td>Візуально: ліс краще неба</td>
                        </tr>
                        <tr>
                            <td>Формат</td>
                            <td>PNG або BMP (lossless)</td>
                            <td>Розширення файлу</td>
                        </tr>
                        <tr>
                            <td>Стиснення</td>
                            <td>Не стиснуте раніше</td>
                            <td>Перевірити EXIF: оригінал з камери</td>
                        </tr>
                        <tr>
                            <td>Різноманітність</td>
                            <td>Кілька типів об'єктів</td>
                            <td>Краще: місто > окремий будинок</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Порада 2: Швидка оцінка придатності зображення</h3>

            <div class="step-list">
                <ol>
                    <li>
                        <strong>Відкрийте зображення в будь-якому редакторі</strong>
                    </li>
                    <li>
                        <strong>Перевірте гістограму</strong><br>
                        - Гарно: широкий розподіл по всьому діапазону 0-255<br>
                        - Погано: пік на одному значенні (гладке зображення)
                    </li>
                    <li>
                        <strong>Застосуйте фільтр "Detect edges" (виявлення ребер)</strong><br>
                        - Гарно: багато ребер по всьому зображенню (≥40% пікселів)<br>
                        - Погано: мало ребер (&lt;20% пікселів)
                    </li>
                    <li>
                        <strong>Швидкий висновок:</strong><br>
                        Якщо гістограма широка + багато ребер → зображення придатне!
                    </li>
                </ol>
            </div>

            <h3>Порада 3: Оптимальний розподіл payload</h3>

            <div class="info-box">
                <h4>📊 Правило "30-20-10"</h4>
                <p>Для максимальної безпеки дотримуйтесь цих лімітів:</p>
                <ul>
                    <li><strong>0.3 bpp</strong> — максимум для регулярного використання (точність виявлення ~62-65%)</li>
                    <li><strong>0.2 bpp</strong> — рекомендовано для важливих даних (точність ~57-60%)</li>
                    <li><strong>0.1 bpp</strong> — для максимальної безпеки (точність ~52-55%, майже невиявний)</li>
                </ul>

                <p><strong>Приклад розрахунку:</strong></p>
                <div class="code-block" style="background: #f8fafc; color: #1e293b; padding: 1rem;">
Зображення: 1024 × 768 = 786,432 пікселі

Максимальні безпечні payload:
- 0.3 bpp: 786,432 × 0.3 = 235,930 біт ≈ 29 KB
- 0.2 bpp: 786,432 × 0.2 = 157,286 біт ≈ 19 KB
- 0.1 bpp: 786,432 × 0.1 = 78,643 біт ≈ 9.8 KB

Якщо ваш файл 50 KB:
→ Використайте 2-3 зображення (по 20-25 KB на кожне)
                </div>
            </div>

            <h3>Порада 4: Комбінування з криптографією</h3>

            <div class="important-box">
                <h4>🔐 Подвійний захист</h4>
                <p>
                    <strong>Завжди шифруйте повідомлення перед вбудовуванням!</strong>
                </p>

                <p><strong>Чому це важливо:</strong></p>
                <ul>
                    <li>Стеганографія ховає <em>існування</em> повідомлення</li>
                    <li>Криптографія ховає <em>зміст</em> повідомлення</li>
                    <li>Разом: навіть якщо стегоконтейнер виявлено, дані все одно захищені!</li>
                </ul>

                <p><strong>Рекомендована схема:</strong></p>
                <div class="step-list">
                    <ol>
                        <li>Шифрування: AES-256-GCM (повідомлення → зашифровані біти)</li>
                        <li>Стеганографія: S-UNIWARD (зашифровані біти → зображення)</li>
                        <li>Передача: публікація зображення</li>
                        <li>Отримувач: Витягування (S-UNIWARD) → Дешифрування (AES)</li>
                    </ol>
                </div>
            </div>

            <h3>Порада 5: Пакетна обробка для швидкості</h3>

            <div class="example-box">
                <h4>⚡ Прискорення обробки багатьох зображень</h4>

                <p><strong>Проблема:</strong> Потрібно вбудувати дані в 100 зображень. WOW займає 3 сек на зображення = 5 хвилин загалом.</p>

                <p><strong>Рішення: Паралельна обробка</strong></p>
                <ul>
                    <li>Використати багатопоточність (Python: multiprocessing, threading)</li>
                    <li>4 потоки → 4 зображення одночасно → час зменшується в 4 рази!</li>
                    <li>100 зображень: з 5 хвилин до ~1.5 хвилини</li>
                </ul>

                <p><strong>Додатково:</strong> GPU прискорення для обчислення вейвлет-перетворень може дати ще 2-3× прискорення</p>
            </div>

        </section>

        <!-- Quick Reference -->
        <section id="reference">
            <h2>Додаток 5: Швидкий довідник</h2>

            <h3>Порівняльна таблиця: Вибір методу</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Ситуація</th>
                        <th>Рекомендований метод</th>
                        <th>Чому</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Максимальна безпека, payload &lt; 0.4 bpp</td>
                        <td><strong>S-UNIWARD</strong></td>
                        <td>Найнижча виявлюваність (62%)</td>
                    </tr>
                    <tr>
                        <td>Швидкість критична (real-time)</td>
                        <td><strong>WOW</strong></td>
                        <td>Найшвидший (3 сек vs 18 у HUGO)</td>
                    </tr>
                    <tr>
                        <td>Обмежена пам'ять (mobile, embedded)</td>
                        <td><strong>WOW</strong></td>
                        <td>Найменші вимоги (50 MB)</td>
                    </tr>
                    <tr>
                        <td>Робота з JPEG файлами</td>
                        <td><strong>J-UNIWARD</strong></td>
                        <td>Версія UNIWARD для JPEG domain</td>
                    </tr>
                    <tr>
                        <td>Академічні дослідження</td>
                        <td><strong>HUGO</strong></td>
                        <td>Класика, benchmark</td>
                    </tr>
                    <tr>
                        <td>Production система, різні формати</td>
                        <td><strong>S-UNIWARD</strong></td>
                        <td>Універсальність + безпека</td>
                    </tr>
                    <tr>
                        <td>Пакетна обробка тисяч зображень</td>
                        <td><strong>WOW</strong></td>
                        <td>Швидкість + good enough безпека</td>
                    </tr>
                    <tr>
                        <td>Дуже малий payload (&lt; 0.1 bpp)</td>
                        <td><strong>Будь-який</strong></td>
                        <td>При малому payload всі показують ~50-52%</td>
                    </tr>
                </tbody>
            </table>

            <h3>Таблиця лімітів payload для безпеки</h3>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Розмір зображення</th>
                        <th>0.1 bpp (безпечно)</th>
                        <th>0.2 bpp (добре)</th>
                        <th>0.3 bpp (ліміт)</th>
                        <th>0.4 bpp (ризиковано)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>512 × 512</td>
                        <td>3.2 KB</td>
                        <td>6.5 KB</td>
                        <td>9.8 KB</td>
                        <td>13 KB</td>
                    </tr>
                    <tr>
                        <td>1024 × 768</td>
                        <td>9.6 KB</td>
                        <td>19 KB</td>
                        <td>29 KB</td>
                        <td>38 KB</td>
                    </tr>
                    <tr>
                        <td>1920 × 1080 (Full HD)</td>
                        <td>25 KB</td>
                        <td>51 KB</td>
                        <td>77 KB</td>
                        <td>103 KB</td>
                    </tr>
                    <tr>
                        <td>3840 × 2160 (4K)</td>
                        <td>101 KB</td>
                        <td>202 KB</td>
                        <td>303 KB</td>
                        <td>404 KB</td>
                    </tr>
                </tbody>
            </table>

            <h3>Чек-лист перед використанням</h3>

            <div class="info-box">
                <h4>✅ Перевірте перед вбудовуванням:</h4>
                <ul style="list-style: none; padding-left: 0;">
                    <li>☐ Зображення має розмір ≥ 512×512</li>
                    <li>☐ Формат lossless (PNG/BMP) або використовується J-UNIWARD для JPEG</li>
                    <li>☐ Зображення не використовувалось раніше</li>
                    <li>☐ Payload &lt; 0.3 bpp</li>
                    <li>☐ Повідомлення зашифроване (AES-256)</li>
                    <li>☐ Метод обрано відповідно до вимог (безпека/швидкість)</li>
                    <li>☐ Код протестовано на тестових даних</li>
                    <li>☐ PSNR перевірено (&gt; 45 dB)</li>
                    <li>☐ Витягування працює коректно (roundtrip test)</li>
                    <li>☐ Результат перевірено детектором (опціонально, але рекомендовано)</li>
                </ul>
            </div>

            <h3>Корисні ресурси</h3>

            <div class="info-box">
                <h4>🔗 Посилання для подальшого вивчення</h4>

                <p><strong>Офіційні реалізації:</strong></p>
                <ul>
                    <li>DDE Lab (Binghamton University): <code>dde.binghamton.edu/download/stego_algorithms/</code></li>
                    <li>BOSSbase (тестова база): <code>agents.fel.cvut.cz/boss/</code></li>
                </ul>

                <p><strong>Інструменти для тестування:</strong></p>
                <ul>
                    <li>Aletheia (Python steganalysis tool): <code>github.com/daniellerch/aletheia</code></li>
                    <li>StegExpose (детектор): <code>github.com/b3dk7/StegExpose</code></li>
                </ul>

                <p><strong>Наукові статті (must-read):</strong></p>
                <ul>
                    <li>HUGO: Pevný et al., 2010 — "Using High-Dimensional Image Models..."</li>
                    <li>WOW: Holub & Fridrich, 2012 — "Designing Steganographic Distortion..."</li>
                    <li>S-UNIWARD: Holub et al., 2014 — "Universal Distortion Function..."</li>
                </ul>

                <p><strong>Книги:</strong></p>
                <ul>
                    <li>Jessica Fridrich — "Steganography in Digital Media" (2009)</li>
                </ul>
            </div>

        </section>

    </article>

    <!-- Back Button -->
    <div style="text-align: center; margin: 3rem 0;">
        <a href="index.html" class="btn btn-primary" style="display: inline-block; padding: 1rem 2rem; background: #667eea; color: white; text-decoration: none; border-radius: 8px; font-size: 1.1rem;">
            ← Повернутися до лекції 7
        </a>
    </div>

    </main>

</div>

<!-- Back to Top Button -->
<button class="back-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'})" aria-label="Повернутися вгору">
    ↑
</button>

<!-- Footer -->
<footer class="footer">
    <div class="container">
        <div class="footer-content text-center">
            <p>&copy; 2025 Курс "Основи стеганографії" | Кафедра кібербезпеки</p>
        </div>
    </div>
</footer>

<script>
    // Active section highlighting
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('.sidebar-nav a');

    function updateActiveNav() {
        let currentSection = '';

        sections.forEach(section => {
            const sectionTop = section.offsetTop;
            const sectionHeight = section.clientHeight;
            if (window.scrollY >= (sectionTop - 150)) {
                currentSection = section.getAttribute('id');
            }
        });

        navLinks.forEach(link => {
            link.classList.remove('active');
            if (link.getAttribute('href') === `#${currentSection}`) {
                link.classList.add('active');
            }
        });
    }

    // Reading progress bar
    const progressBar = document.querySelector('.reading-progress-bar');

    function updateReadingProgress() {
        const windowHeight = window.innerHeight;
        const documentHeight = document.documentElement.scrollHeight;
        const scrollTop = window.scrollY;
        const scrollPercentage = (scrollTop / (documentHeight - windowHeight)) * 100;
        progressBar.style.width = `${Math.min(scrollPercentage, 100)}%`;
    }

    // Back to top button visibility
    const backToTopBtn = document.querySelector('.back-to-top');

    function toggleBackToTop() {
        if (window.scrollY > 500) {
            backToTopBtn.classList.add('show');
        } else {
            backToTopBtn.classList.remove('show');
        }
    }

    // Event listeners
    window.addEventListener('scroll', () => {
        updateActiveNav();
        updateReadingProgress();
        toggleBackToTop();
    });

    // Initial calls
    updateActiveNav();
    updateReadingProgress();
    toggleBackToTop();
</script>

</body>
</html>
